{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Keras\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lesson, we will explore image classification using a **Convolutional Neural Network (CNN)** in **Keras** with **TensorFlow**. Keras is a high-level API built on top of TensorFlow, designed to simplify the process of building and training neural networks. TensorFlow, on the other hand, is a comprehensive open-source machine learning framework that provides powerful tools for building and deploying deep learning models, including neural networks.\n",
    "\n",
    "We will use a dataset of benthic animals to classify images of **crabs** and **rockfish**, which were captured in 2011 by the **ROV ROPOS** at **Southern Hydrate Ridge**, as part of the **Ocean Observatories Initiative Regional Cabled Array** operated by the University of Washington and funded by the National Science Foundation (NSF).\n",
    "\n",
    "### Object Detection vs. Image Classification\n",
    "\n",
    "There are two common tasks in computer vision: **object detection** and **image classification**.\n",
    "\n",
    "- **Image classification** involves categorizing an entire image into a single class. In this case, we are classifying each image as either a \"crab\" or a \"rockfish.\"\n",
    "  \n",
    "- **Object detection**, on the other hand, is more complex. It involves not only identifying the objects present in an image but also determining their exact location within the image. This requires the use of annotations (usually separate files) to provide bounding boxes around each object.\n",
    "\n",
    "In this lesson, we focus on **image classification**. The images used in this dataset are not annotated with bounding boxes because they are assumed to contain only one of the two possible classes—either \"crab\" or \"rockfish.\" This simplifies the problem, allowing us to rely on the image file names and folder structure to determine the class labels. No separate annotation files are necessary.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand how to load, preprocess, and split image datasets for training a neural network.\n",
    "2. Develop, compile, and train a Convolutional Neural Network (CNN) model for image classification using Keras.\n",
    "3. Start to visualize and interpret training and validation accuracy and loss graphs, and evaluate model performance.\n",
    "4. Make predictions on new, unseen images and assess the model's confidence in classifying them.\n",
    "\n",
    ":::{note}\n",
    "The following activity requires a small dataset download, you can download it here: \n",
    ":::\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Overview\n",
    "\n",
    "Here’s a quick overview of the libraries used in this lesson:\n",
    "\n",
    "- **matplotlib.pyplot**: A plotting library used to visualize data. In this lesson, it helps display images and graphs of model performance.\n",
    "- **numpy**: A fundamental library for numerical operations in Python, often used for handling arrays and matrices.\n",
    "- **PIL (Python Imaging Library)**: Provides functionality for opening, manipulating, and saving image files. In this lesson, it's used to display sample images.\n",
    "- **OpenCV (cv2)**: Provides advanced image augmentation\n",
    "- **tensorflow**: A popular machine learning framework used for building and training models. We use TensorFlow’s high-level Keras API to create and train the Convolutional Neural Network (CNN) in this lesson.\n",
    "- **random**: A standard Python module used here to randomly select and display images from the dataset.\n",
    "- **pathlib**: Used for handling and manipulating file system paths in an easy-to-use manner.\n",
    "- **zipfile**: A Python library for handling `.zip` files. Here, it’s used to extract the dataset.\n",
    "- **tensorflow.keras**: A high-level neural networks API, included with TensorFlow, used to create layers and models for machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pathlib\n",
    "import zipfile\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "This tutorial uses a dataset of **800 photos** of benthic animals, consisting of two classes:\n",
    "\n",
    "- **crab**\n",
    "- **rockfish**\n",
    "\n",
    "These images are organized into two subdirectories within the dataset folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "new_benthic_photo/\n",
    "  crab/\n",
    "  rockfish/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract and Load Dataset\n",
    "\n",
    "dataset_path = \"/content/SHRCrabsandFishClassification.zip\"\n",
    "\n",
    "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/content/new_benthic_photo\")\n",
    "\n",
    "data_dir = pathlib.Path(\"/content/new_benthic_photo\")\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "print(f'Total number of images: {image_count}')\n",
    "\n",
    "crabs = list(data_dir.glob('crab/*'))\n",
    "rockfish = list(data_dir.glob('rockfish/*'))\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "Image.open(str(crabs[0])).show()\n",
    "Image.open(str(crabs[1])).show()\n",
    "Image.open(str(rockfish[0])).show()\n",
    "Image.open(str(rockfish[1])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define Data Directories\n",
    "\n",
    "crab_dir = data_dir / 'crab'\n",
    "rockfish_dir = data_dir / 'rockfish'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{note}\n",
    "By defining the data directories for each class (crab and rockfish), we make it easier to access and manage the images belonging to each category. This allows us to quickly refer to the directories when performing operations like random sampling, image preprocessing, or displaying specific examples from each class. This is a bit overkill with only 2 classes, but its good practice because it ensures that our code is organized and can efficiently work with large datasets where images are separated into folders based on their classes.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Display Random Images from the dataset\n",
    "\n",
    "crab = list(data_dir.glob('crab/*'))\n",
    "random_crab = random.randint(0, min(len(crabs), 400) - 1)\n",
    "PIL.Image.open(str(crab[random_crab])).show()\n",
    "\n",
    "rockfish = list(data_dir.glob('rockfish/*'))\n",
    "random_rockfish = random.randint(0, min(len(rockfish), 400) - 1)\n",
    "PIL.Image.open(str(rockfish[random_rockfish])).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Understanding Dataset Parameters\n",
    "\n",
    "Here, we set key parameters for handling the dataset:\n",
    "\n",
    "- **batch_size = 32**: This defines how many images will be processed in one pass through the model. Using a batch size of 32 means that during training, the model will look at 32 images before updating its internal parameters. A batch size of 32 is commonly used for balancing memory efficiency and training speed.\n",
    "\n",
    "- **img_height = 180** and **img_width = 180**: These define the dimensions to which each image will be resized. By resizing all images to a uniform height and width of 180x180 pixels, we ensure consistency in input size, which is required for neural networks. Although resizing reduces detail, it also speeds up computation and simplifies model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set Parameters for the Dataset\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "We use the `tf.keras.utils.image_dataset_from_directory` function to load and preprocess the dataset. Here's what each argument does:\n",
    "\n",
    "- **data_dir**: This points to the directory where the dataset is stored (the root folder containing the `crab` and `rockfish` subdirectories).\n",
    "  \n",
    "- **validation_split = 0.2**: This splits the dataset into training and validation sets. In this case, 80% of the data will be used for training, and 20% for validation.\n",
    "  \n",
    "- **subset**: We specify whether we're creating the **training** or **validation** dataset. Setting `subset=\"training\"` creates the training set, and `subset=\"validation\"` creates the validation set.\n",
    "\n",
    "- **seed = 123**: A seed value ensures that the dataset split is reproducible, meaning that the same data will always be allocated to the training and validation sets when the code is rerun.\n",
    "\n",
    "- **image_size = (img_height, img_width)**: This resizes each image to the predefined size of `180x180` pixels, ensuring all images are consistent when input to the model.\n",
    "\n",
    "- **batch_size = 32**: This defines the number of images processed in each batch, ensuring efficient training and memory usage.\n",
    "\n",
    "This setup loads the dataset in a format that's ready for model training while automatically handling preprocessing like image resizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Display Class Names\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "This block of code is used to visualize a sample of images from the training dataset. Here's what each part does:\n",
    "\n",
    "- **plt.figure(figsize=(10, 10))**: This sets up the figure size for displaying the images. The size `(10, 10)` ensures a large enough grid to comfortably view multiple images.\n",
    "\n",
    "- **train_ds.take(1)**: This grabs a single batch of images and labels from the training dataset. Since the batch size is set to 32, this will retrieve 32 images and their corresponding labels, but we're only displaying 9 of them.\n",
    "\n",
    "- **for i in range(9)**: This loop goes through the first 9 images in the batch and plots them.\n",
    "\n",
    "- **ax = plt.subplot(3, 3, i + 1)**: This creates a `3x3` grid of subplots to display 9 images.\n",
    "\n",
    "- **plt.imshow(images[i].numpy().astype(\"uint8\"))**: This converts each image tensor into a NumPy array and displays it as an image.\n",
    "\n",
    "- **plt.title(class_names[labels[i]])**: This adds the class name (either \"crab\" or \"rockfish\") as the title above each image based on the label associated with the image.\n",
    "\n",
    "- **plt.axis(\"off\")**: This hides the axes for a cleaner visualization of the images.\n",
    "\n",
    "The purpose of this block is to quickly visualize how the dataset looks, allowing us to verify that the images and their labels are being loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Dataset Structure\n",
    "\n",
    "In this block of code, we inspect the structure of the dataset by printing the shape of a batch of images and their corresponding labels. Before diving into the code, let's briefly discuss **tensors**.\n",
    "\n",
    "### What Are Tensors?\n",
    "\n",
    "A **tensor** is a multi-dimensional array that generalizes matrices to higher dimensions. Tensors are the basic data structure in deep learning and are used to represent inputs, weights, and outputs of models. In our case, each image is represented as a 3D tensor (height, width, color channels), and the dataset is organized into batches of these tensors. The labels are 1D tensors representing the class of each image.\n",
    "\n",
    "### Code Breakdown:\n",
    "\n",
    "- **for image_batch, labels_batch in train_ds**: This loop retrieves a single batch of images and labels from the training dataset. Since our `batch_size` is 32, the batch will contain 32 images and their corresponding labels.\n",
    "\n",
    "- **print(image_batch.shape)**: This prints the shape of the `image_batch`, which should be `(32, 180, 180, 3)`:\n",
    "  - `32`: The batch size (32 images).\n",
    "  - `180, 180`: The dimensions of each image, resized to 180x180 pixels.\n",
    "  - `3`: The number of color channels (RGB).\n",
    "\n",
    "- **print(labels_batch.shape)**: This prints the shape of the `labels_batch`, which should be `(32,)` because there are 32 labels, one for each image in the batch.\n",
    "\n",
    "- **break**: This ensures that the loop runs only once, as we only need to check the structure of one batch.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "This step is important to confirm that the dataset is loaded correctly and the images and labels are properly batched and shaped, making them ready for input into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify Dataset Structure\n",
    "\n",
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## A Basic Keras Model\n",
    "\n",
    "In this section, we define the **Convolutional Neural Network (CNN)** architecture for image classification. Unlike the previous lesson where we used a single layer with an **edge detection kernel**, here we employ a deeper network with multiple layers. Each layer plays a crucial role in feature extraction and learning from the dataset. Let’s go through each part of this code in detail.\n",
    "\n",
    "### Dataset Preparation for Efficient Processing\n",
    "\n",
    "We use `AUTOTUNE` to optimize the data loading and processing, and we configure the training and validation datasets for efficient shuffling, caching, and prefetching.\n",
    "\n",
    "### Normalization Layer\n",
    "\n",
    "The normalization layer rescales pixel values from the range `[0, 255]` to `[0, 1]`. Normalizing the input helps the model converge faster during training by ensuring that the pixel values are small and consistent.\n",
    "\n",
    "### Model Structure: Sequential API\n",
    "\n",
    "The **Sequential API** allows us to stack layers in a linear fashion to define a basic CNN architecture.\n",
    "\n",
    "#### Input Layer\n",
    "\n",
    "The input layer first rescales the images and specifies that each input image has dimensions `180x180` and 3 color channels (RGB).\n",
    "\n",
    "#### First Convolutional Block\n",
    "\n",
    "The first block consists of a **Conv2D layer** with 16 filters (or kernels) followed by **MaxPooling**. The convolutional layer applies filters to the input image to extract low-level features (like edges), while the pooling layer reduces the spatial dimensions of the feature maps, making the model less sensitive to small changes and reducing the computational cost.\n",
    "\n",
    "#### Second Convolutional Block\n",
    "\n",
    "The second block has 32 filters, allowing the network to learn more complex features like textures and patterns. Max pooling again reduces the dimensions of the output.\n",
    "\n",
    "#### Third Convolutional Block\n",
    "\n",
    "In the third block, 64 filters are applied, allowing the model to detect higher-level features. The deeper we go into the network, the more abstract the features become, allowing the model to make more complex distinctions between classes.\n",
    "\n",
    "#### Flattening and Dense Layers\n",
    "\n",
    "The **Flatten** layer converts the 2D feature maps into a 1D vector that is fed into the dense layers. The **Dense(128)** layer learns to combine the features detected in the convolutional layers. The final **Dense(num_classes)** layer outputs the classification logits for the `crab` and `rockfish` classes.\n",
    "\n",
    "### Layer Importance\n",
    "\n",
    "In the previous lesson, we used a single **edge detection kernel**, which was a manual, handcrafted filter. Here, the CNN automatically learns the best filters (or features) from the data. The layer structure is key to CNNs because each layer extracts more complex features, allowing the network to progressively understand the image in a more detailed and abstract way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A Basic Keras Model\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "\n",
    "After defining the architecture of the CNN, the next step is to **compile** the model. Compiling involves specifying three key elements: the **optimizer**, the **loss function**, and the **metrics** used to evaluate the model’s performance.\n",
    "\n",
    "### Optimizer: Adam\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** is a popular optimizer that combines the advantages of two other methods: **AdaGrad** (which works well with sparse gradients) and **RMSProp** (which adapts the learning rate based on recent gradients). Adam adapts the learning rate throughout training, making it a good default choice for most models.\n",
    "\n",
    "### Loss Function: Sparse Categorical Crossentropy\n",
    "\n",
    "**Sparse Categorical Crossentropy** is the loss function used when we have multiple classes (in this case, `crab` and `rockfish`), and the labels are integer values. It measures how far the predicted probabilities are from the actual labels.\n",
    "\n",
    "- **from_logits=True**: This indicates that the output of the final layer is raw scores (logits) rather than probabilities. Since we haven’t applied a softmax activation in the final layer, logits will be converted to probabilities during the loss calculation.\n",
    "\n",
    "### Metrics: Accuracy\n",
    "\n",
    "**Accuracy** calculates the percentage of correct predictions made by the model. During training, both **training accuracy** and **validation accuracy** will be tracked to monitor how well the model is learning and generalizing.\n",
    "\n",
    "By compiling the model, we set the stage for training, specifying how the model will optimize its weights, calculate loss, and measure success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary\n",
    "\n",
    "The `model.summary()` function provides a concise overview of the model’s architecture. It displays the layer types, output shapes, and the number of parameters in each layer, giving you a quick understanding of the model’s structure and complexity.\n",
    "\n",
    "This is useful to verify that the model is constructed as intended and to check the total number of trainable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "In this step, we train the model using the `model.fit()` function. \n",
    "\n",
    "- **epochs=10**: This specifies that the model will go through the entire dataset 10 times (or 10 training cycles). Each epoch allows the model to learn more patterns and improve its predictions.\n",
    "- **train_ds**: The training dataset used for learning.\n",
    "- **validation_data=val_ds**: The validation dataset used to evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "The **history** object stores the training progress, including accuracy and loss, which we visualize in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Results\n",
    "\n",
    "This block of code visualizes the model’s **training and validation accuracy** and **loss** over the training epochs. These plots are crucial for understanding how well the model is learning and generalizing.\n",
    "\n",
    "- **Training Accuracy and Validation Accuracy**: These graphs show how the model's accuracy improves during training and how well it performs on unseen validation data.\n",
    "- **Training Loss and Validation Loss**: These graphs show how the model’s loss decreases during training. Loss is a measure of how well the model’s predictions match the true labels.\n",
    "\n",
    "Interpreting these graphs is one of the most important skills in CV. They help you assess whether the model is **overfitting** (performing well on training but poorly on validation) or **underfitting** (performing poorly on both training and validation). \n",
    "\n",
    "The next section will cover different metrics in greater depth and what constitutes good training results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize Training Results\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on a New Image\n",
    "\n",
    "After training the model, we can use it to make predictions on new, unseen images. In this example, we load a new image called **\"mystery.png\"**, preprocess it, and let the model predict whether it belongs to the \"crab\" or \"rockfish\" class. \n",
    "\n",
    "Here’s a breakdown:\n",
    "- **Load the image**: The image is resized to the same dimensions (180x180) used during training.\n",
    "- **Preprocess**: The image is converted to an array and expanded to match the input shape expected by the model (batch format).\n",
    "- **Make predictions**: The model generates raw prediction scores for each class. We apply the **softmax** function to convert these scores into probabilities.\n",
    "- **Print the result**: We output the predicted class (either \"crab\" or \"rockfish\") along with the confidence level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on \"mystery.png\"\n",
    "\n",
    "image_path = \"/content/mystery.png\"\n",
    "\n",
    "# Load the image\n",
    "img_height = 180  # Set the appropriate target size\n",
    "img_width = 180\n",
    "img = tf.keras.utils.load_img(image_path, target_size=(img_height, img_width))\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "class_names = ['crab', 'rockfish']\n",
    "\n",
    "# Print the result\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
