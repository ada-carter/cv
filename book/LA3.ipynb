{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: The Math Behind Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lesson, we will explore the basic mathematical concepts that form the foundation of **Convolutional Neural Networks (CNNs)**, one of the most popular models used in **Computer Vision**. We will discuss concepts such as **convolutions**, **filters**, **stride**, and **padding**, and how these operations help in extracting meaningful features from images for tasks like object detection, classification, and segmentation.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "- Understand the basic mathematical operations behind CNNs, such as convolutions and filters.\n",
    "- Learn how CNNs process image data using operations like stride and padding.\n",
    "- Explore how CNNs build hierarchical features from simple edges to complex shapes.\n",
    "---\n",
    "\n",
    "## Mathematical Foundations of CNNs\n",
    "\n",
    "Convolutional Neural Networks rely on mathematical operations that process image data in layers. The key operation is **convolution**, which is used to detect features in the input images.\n",
    "\n",
    "### Convolutions\n",
    "\n",
    "A convolution operation is essentially a way of applying a filter (kernel) to an image. The kernel slides over the input image, multiplying and summing the values to produce an output.\n",
    "\n",
    "#### Convolution Formula\n",
    "\n",
    "Let’s represent the convolution operation mathematically:\n",
    "Given an image `I` and a filter `F`, the convolution at a position `(x, y)` can be expressed as:\n",
    "\n",
    "\\[\n",
    "S(x, y) = \\sum_{i=0}^{k-1} \\sum_{j=0}^{k-1} I(x+i, y+j) \\cdot F(i,j)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- `I(x, y)` is the pixel value at position `(x, y)` in the input image.\n",
    "- `F(i, j)` is the value of the filter at position `(i, j)`.\n",
    "- `S(x, y)` is the output (or feature map) after applying the filter.\n",
    "\n",
    "### Filters\n",
    "\n",
    "Filters (also called kernels) are small matrices of weights that are applied to input images to extract features such as edges, textures, and more. For example, a 3x3 filter applied to an image can help detect vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple vertical edge detection filter\n",
    "edge_filter = np.array([[1, 0, -1],\n",
    "                        [1, 0, -1],\n",
    "                        [1, 0, -1]])\n",
    "\n",
    "# Applying this filter in a CNN will detect vertical edges in images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride and Padding\n",
    "\n",
    "Two other important concepts in CNNs are **stride** and **padding**.\n",
    "\n",
    "- **Stride**: Stride defines how the filter moves across the image. A stride of 1 means the filter moves one pixel at a time, whereas a stride of 2 skips every other pixel.\n",
    "  \n",
    "  Mathematically, if the stride is `s`, the output size after convolution is:\n",
    "  \n",
    "  \\[\n",
    "  \\text{Output Size} = \\frac{(N - F)}{s} + 1\n",
    "  \\]\n",
    "  \n",
    "  Where `N` is the input size and `F` is the filter size.\n",
    "\n",
    "- **Padding**: Padding refers to adding extra pixels (usually zeros) around the borders of the image. Padding helps preserve the spatial dimensions of the image after convolution, especially when filters reduce the size of the image.\n",
    "\n",
    "### Pooling\n",
    "\n",
    "Pooling is a downsampling operation used to reduce the spatial size of the feature maps. The most common type of pooling is **max pooling**, which takes the maximum value from a portion of the image.\n",
    "\n",
    "For example, in 2x2 max pooling:\n",
    "\n",
    "\\[\n",
    "P(x, y) = \\max \\{I(x, y), I(x+1, y), I(x, y+1), I(x+1, y+1)\\}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Building Hierarchical Features\n",
    "\n",
    "CNNs learn hierarchical features through multiple convolutional layers. The first layers detect simple edges or colors, while deeper layers capture more complex structures like shapes or patterns.\n",
    "\n",
    "### Example Code in PyTorch\n",
    "\n",
    "Below is a simple example of how a convolution operation is implemented in **PyTorch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a convolutional layer\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Example input (single-channel image)\n",
    "input_image = torch.randn(1, 1, 5, 5)\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv_layer(input_image)\n",
    "\n",
    "print(\"Input Image:\\n\", input_image)\n",
    "print(\"Convolved Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we defined a convolutional layer with a 3x3 filter, a stride of 1, and padding to preserve the image size.\n",
    "\n",
    "## Let's Move onto a More Complex Example\n",
    "\n",
    "This example can look daunting at first, but don’t worry, we will walk through it step by step. The purpose of this activity is to give you a practical example of how to apply the convolution techniques you’ve learned on an actual image. By the end of this exercise, you will understand how to upload an image, apply a convolution (edge detection), and display both the original and processed images side by side.\n",
    "\n",
    "### Breakdown of the Code Blocks\n",
    "\n",
    "In this section, we will dissect each part of the code to understand its purpose.\n",
    "\n",
    "### Block 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is essential for setting up the environment. Here's a breakdown of the imports:\n",
    "\n",
    "- `torch` and `torch.nn` are part of the PyTorch library, which we will use to define and apply the convolutional filter.\n",
    "- `matplotlib.pyplot` will help us visualize the images.\n",
    "- `numpy` is used to handle arrays, which is the format that images take when processed.\n",
    "- `PIL.Image` allows us to load and manipulate images.\n",
    "- `google.colab.files` is a utility that allows us to upload files directly into Google Colab.\n",
    "\n",
    "### Block 2: Uploading the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block prompts you to upload an image. It uses Colab’s `files.upload()` function to allow users to select and upload a file. For this exercise, we are working with a **180x180 grayscale image of a crab**. Once the image is uploaded, the file is stored in memory for further processing.\n",
    "\n",
    "### Block 3: Opening and Preprocessing the Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = next(iter(uploaded))\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "image = image.resize((180, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we open the uploaded image and convert it to grayscale using `.convert('L')`, where `'L'` means luminance or grayscale. The image is resized to **180x180 pixels** to ensure it's the correct size for our convolution operation.\n",
    "\n",
    "- **Why grayscale?** CNNs typically process grayscale or single-channel images more efficiently for basic operations like edge detection, as it simplifies the data (reducing from 3 RGB channels to 1).\n",
    "- **Resizing** ensures that the image fits within the expected dimensions of the neural network input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 4: Converting the Image to a PyTorch Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_np = np.array(image)\n",
    "image_tensor = torch.tensor(image_np, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block first converts the image from a PIL format to a NumPy array using `np.array(image)`. Then, it transforms the NumPy array into a PyTorch tensor, which is the format used by PyTorch for processing data.\n",
    "\n",
    "- The `.unsqueeze(0)` is applied twice to add two dimensions: one for the **batch size** and one for the **channel** (in this case, just one channel for grayscale images).\n",
    "- **Why tensors?** Tensors are the data structure that PyTorch uses for computation, similar to arrays in NumPy but optimized for GPU operations.\n",
    "\n",
    "### Block 5: Defining the Edge Detection Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "edge_filter = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "edge_filter.weight = torch.nn.Parameter(torch.tensor([[[[1, 0, -1], [1, 0, -1], [1, 0, -1]]]]))  # Vertical edge filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block defines a **convolutional layer** (`Conv2d`) with a 3x3 kernel (filter) that will be used for detecting vertical edges.\n",
    "\n",
    "- `in_channels=1` and `out_channels=1`: This means we have a single input (grayscale image) and a single output.\n",
    "- `kernel_size=3`: Specifies that we are using a 3x3 filter.\n",
    "- `stride=1`: This means the filter moves one pixel at a time.\n",
    "- `padding=1`: Adds zero-padding around the image to ensure the output size matches the input size.\n",
    "\n",
    "The filter defined here is a **vertical edge detection filter**, where the kernel looks like this:\n",
    "\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "This filter emphasizes vertical edges by detecting differences in pixel intensities between adjacent vertical pixels.\n",
    "\n",
    "### Block 6: Applying the Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    filtered_image_tensor = edge_filter(image_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, we apply the convolution operation using the edge detection filter on the input image.\n",
    "\n",
    "- **`torch.no_grad()`**: This disables gradient calculation, which is unnecessary here since we are only applying the filter and not training a model.\n",
    "- **Why convolution?** Convolution applies the filter to the image, highlighting specific features (in this case, vertical edges). It scans through the image and performs element-wise multiplication with the filter, followed by summing the results to produce the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 7: Converting the Filtered Image for Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "filtered_image_np = filtered_image_tensor.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 8: Displaying the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Original Image\n",
    "ax[0].imshow(image_np, cmap='gray')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "# Filtered (Edge-detected) Image\n",
    "ax[1].imshow(filtered_image_np, cmap='gray')\n",
    "ax[1].set_title(\"Filtered (Edge-detected) Image\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final block uses Matplotlib to display the images side by side.\n",
    "\n",
    "- **`plt.subplots(1, 2, figsize=(10, 5))`**: Creates two subplots, one for the original image and one for the edge-detected (filtered) image.\n",
    "- The **original image** is shown in the first subplot, while the **filtered image** (after applying the edge detection filter) is displayed in the second.\n",
    "\n",
    "This visual comparison allows you to see how the convolutional filter modifies the image by highlighting vertical edges, reinforcing the math behind convolutions you’ve learned.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now lets put it all together, open the whole code snippet below in google colab and test out working in that environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the image (make sure it's the provided 180x180 image of a crab for this exercise)\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Open the uploaded image\n",
    "image_path = next(iter(uploaded))\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "image = image.resize((180, 180))\n",
    "\n",
    "# Convert image to numpy array and then to PyTorch tensor\n",
    "image_np = np.array(image)\n",
    "image_tensor = torch.tensor(image_np, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Define a simple edge detection filter\n",
    "edge_filter = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "edge_filter.weight = torch.nn.Parameter(torch.tensor([[[[1, 0, -1], [1, 0, -1], [1, 0, -1]]]]))  # Vertical edge filter\n",
    "\n",
    "# Apply the filter (convolution)\n",
    "with torch.no_grad():\n",
    "    filtered_image_tensor = edge_filter(image_tensor)\n",
    "\n",
    "# Convert the filtered image back to numpy for display\n",
    "filtered_image_np = filtered_image_tensor.squeeze().numpy()\n",
    "\n",
    "# Plot the original and filtered images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(image_np, cmap='gray')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "# Filtered (Edge-detected) Image\n",
    "ax[1].imshow(filtered_image_np, cmap='gray')\n",
    "ax[1].set_title(\"Filtered (Edge-detected) Image\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
