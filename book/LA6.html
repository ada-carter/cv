
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>9. LA6: Object Detection with TensorFlow API &#8212; Computer Vision Across Oceanography</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/LA6';</script>
    <link rel="canonical" href="https://atticus-carter.github.io/cv/book/LA6.html" />
    <link rel="icon" href="../_static/fav.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="15. Instance Segmentation with Detectron2" href="LA7.html" />
    <link rel="prev" title="8. Image Classification with Keras" href="LA5.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Computer Vision Across Oceanography - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Computer Vision Across Oceanography - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 1 - Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="LA1.html">1. Introduction to Ocean Image Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA1.5.html">2. Marine Imaging Workshop Survey Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA2.html">3. Image Annotation</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA2.5.html">4. Image Manipulation in Python with PIL and OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA2.75.html">5. Creating and Augmenting Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA3.html">6. The Math Behind Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA4.html">7. Understanding CV Metrics and Graphs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 2 - Computer Vision Development</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="LA5.html">8. Image Classification with Keras</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9. LA6: Object Detection with TensorFlow API</a></li>





<li class="toctree-l1"><a class="reference internal" href="LA7.html">15. Instance Segmentation with Detectron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA8.html">16. Keypoint Detection with MediaPipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA9.html">17. Object and Multi-Object Tracking with TensorFlow and OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA10.html">18. Image Super-Resolution and Enhancement with SRGAN in TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA11.html">19. Self-Supervised Learning for Image Classification with SimCLR in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA12.html">20. Action Recognition and Event Detection in Videos using I3D Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA13.html">21. Anomaly Detection in Images and Videos using Autoencoders and TensorFlow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 3 - Synthesis Project</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="LA14.html">22. Dataset Preparation and Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA15.html">23. Model Selection and Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA16.html">24. Training and Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA17.html">25. Data Visualization and Figure Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA18.html">26. Usecase Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA19.html">27. Tying it all together</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/atticus-carter/cv/master?urlpath=tree/book/LA6.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/atticus-carter/cv/blob/master/book/LA6.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/atticus-carter/cv" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/atticus-carter/cv/edit/main/book/LA6.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/atticus-carter/cv/issues/new?title=Issue%20on%20page%20%2Fbook/LA6.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/book/LA6.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LA6: Object Detection with TensorFlow API</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">9. LA6: Object Detection with TensorFlow API</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">9.1. Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#downloading-the-dataset">9.2. Downloading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-coco-format">9.3. Understanding the COCO Format</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#file-structure">9.3.1. File Structure:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coco-json-structure">9.3.2. COCO JSON Structure:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-environment">9.4. Preparing the Environment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#install-tensorflow-object-detection-api">9.4.1. Install TensorFlow Object Detection API</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-and-extract-the-dataset">9.4.2. Download and Extract the Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">9.4.3. Loading the Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-coco-annotations-to-tensorflow-tfrecord-format">9.4.4. Converting COCO Annotations to TensorFlow TFRecord Format</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-in-object-detection">9.5. Transfer Learning in Object Detection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-transfer-learning">9.5.1. Why Use Transfer Learning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#walking-through-the-code">9.5.2. Walking Through the Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-breakdown">9.5.3. Code Breakdown</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-pipeline-configuration-file">9.5.3.1. 1. <strong>Loading the Pipeline Configuration File</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-configuration-file">9.5.3.2. 2. <strong>Loading the Configuration File</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-detection-model">9.5.3.3. 3. <strong>Building the Detection Model</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-model">9.5.4. Train the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-model">9.5.5. Evaluate the Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflecting-on-the-evaluation-results">9.6. Reflecting on the Evaluation Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-mean-average-precision">9.6.1. <strong>mAP (Mean Average Precision)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-class-map">9.6.2. <strong>Per-Class mAP</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-and-recall">9.6.3. <strong>Precision and Recall</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">9.6.4. <strong>Confusion Matrix</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-values">9.6.5. <strong>Loss Values</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-interpreting-your-results">9.7. Activity: Interpreting Your Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-results">9.7.1. Visualizing Results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#get-the-first-image-from-the-test-directory">10. Get the first image from the test directory</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-first-test-image">11. Load the first test image</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#run-the-detection-model-on-the-image">12. Run the detection model on the image</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-the-detection-results-on-the-image">13. Visualize the detection results on the image</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#display-the-result">14. Display the result</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="la6-object-detection-with-tensorflow-api">
<h1><span class="section-number">9. </span>LA6: Object Detection with TensorFlow API<a class="headerlink" href="#la6-object-detection-with-tensorflow-api" title="Link to this heading">#</a></h1>
<p>In this lesson, we will train an <strong>Object Detection model</strong> using the <strong>TensorFlow Object Detection API</strong>. You’ll be able to choose between two datasets, <strong>“PlasticinWater.coco.zip”</strong> or <strong>“PlasticinWaterNOAUGMENTS.coco.zip”</strong>, based on your system’s capabilities and runtime for Colab sessions. Both datasets are annotated in <strong>COCO format</strong>, with the primary difference being that <strong>PlasticinWater.coco.zip</strong> includes augmented images, whereas <strong>PlasticinWaterNOAUGMENTS.coco.zip</strong> contains the raw images without augmentation.</p>
<hr class="docutils" />
<section id="learning-objectives">
<h2><span class="section-number">9.1. </span>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this section, you will:</p>
<ul class="simple">
<li><p>Understand the structure and format of a dataset in <strong>COCO format</strong>.</p></li>
<li><p>Learn how to load and preprocess a COCO-formatted dataset for object detection.</p></li>
<li><p>Train a custom object detection model using the <strong>TensorFlow Object Detection API</strong> in Google Colab.</p></li>
<li><p>Evaluate model performance using metrics such as <strong>mAP</strong> and visualize results.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="downloading-the-dataset">
<h2><span class="section-number">9.2. </span>Downloading the Dataset<a class="headerlink" href="#downloading-the-dataset" title="Link to this heading">#</a></h2>
<p>You can choose one of the following datasets based on your machine’s ability and the available runtime in Colab:</p>
<ol class="arabic simple">
<li><p><strong>PlasticinWater.coco.zip</strong>:</p>
<ul class="simple">
<li><p><strong>Size</strong>: 10,789 images (with augmentations).</p></li>
<li><p><strong>Description</strong>: Includes a variety of augmented images for a more robust model. Useful if you have more computational power or access to Colab’s extended runtimes.</p></li>
</ul>
</li>
<li><p><strong>PlasticinWaterNOAUGMENTS.coco.zip</strong>:</p>
<ul class="simple">
<li><p><strong>Size</strong>: 4,511 images (without augmentations).</p></li>
<li><p><strong>Description</strong>: Ideal for machines with limited resources or Colab sessions with shorter runtimes.</p></li>
</ul>
</li>
</ol>
<p><strong>Download Links</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://drive.google.com/file/d/1RVPp7EmZ1H3R6IgX2azi0ScKQmNzkp2J/view?usp=sharing">PlasticinWater.coco.zip</a></p></li>
<li><p><a class="reference external" href="https://drive.google.com/file/d/1XO9uzUZsBfNhOS0DXi-W8V2Le80f0srs/view?usp=sharing">PlasticinWaterNOAUGMENTS.coco.zip</a></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="understanding-the-coco-format">
<h2><span class="section-number">9.3. </span>Understanding the COCO Format<a class="headerlink" href="#understanding-the-coco-format" title="Link to this heading">#</a></h2>
<p>The <strong>COCO (Common Objects in Context)</strong> format is a widely used annotation format in object detection tasks. It stores annotations for object detection, segmentation, and keypoint detection in a single <strong>JSON file</strong>. Here’s how the dataset is structured:</p>
<section id="file-structure">
<h3><span class="section-number">9.3.1. </span>File Structure:<a class="headerlink" href="#file-structure" title="Link to this heading">#</a></h3>
<p>Both datasets are structured into <strong>train</strong>, <strong>validation</strong>, and <strong>test</strong> subfolders, each containing:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">_annotations.coco.json</span></code>: The COCO-formatted annotations file for the respective split.</p></li>
<li><p>Images for that split, with annotations stored in the COCO JSON file.</p></li>
</ul>
</section>
<section id="coco-json-structure">
<h3><span class="section-number">9.3.2. </span>COCO JSON Structure:<a class="headerlink" href="#coco-json-structure" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Images</strong>: Contains metadata about each image, such as its ID, file name, and dimensions.</p></li>
<li><p><strong>Annotations</strong>: Contains the <strong>bounding boxes</strong>, <strong>category IDs</strong>, and <strong>segmentation masks</strong> (if any) for each image.</p></li>
<li><p><strong>Categories</strong>: Defines the classes of objects in the dataset (e.g., “plastic”).</p></li>
</ol>
<p><strong>Example</strong>:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;images&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;file_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;000001.jpg&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;height&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">360</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;width&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">640</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;annotations&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;image_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;category_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;bbox&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"> </span><span class="mi">150</span><span class="p">,</span><span class="w"> </span><span class="mi">200</span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;area&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">30000</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;iscrowd&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;categories&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Orange_Plastic_Cap&quot;</span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>bbox: The bounding box coordinates in [x, y, width, height] format.
iscrowd: Determines whether the object is part of a crowd.
area: The area covered by the bounding box.</p>
</section>
</section>
<section id="preparing-the-environment">
<h2><span class="section-number">9.4. </span>Preparing the Environment<a class="headerlink" href="#preparing-the-environment" title="Link to this heading">#</a></h2>
<p>Before training, we’ll set up our environment in Google Colab. Follow these steps to get started:</p>
<section id="install-tensorflow-object-detection-api">
<h3><span class="section-number">9.4.1. </span>Install TensorFlow Object Detection API<a class="headerlink" href="#install-tensorflow-object-detection-api" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clone the TensorFlow Models repository</span>
<span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/tensorflow/models.git

<span class="c1"># Navigate to the research directory</span>
<span class="o">%</span><span class="k">cd</span> models/research/

<span class="c1"># Install the dependencies for TensorFlow Object Detection API</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>object_detection/packages/tf2/setup.py
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>this is a test
</pre></div>
</div>
</div>
</div>
</section>
<section id="download-and-extract-the-dataset">
<h3><span class="section-number">9.4.2. </span>Download and Extract the Dataset<a class="headerlink" href="#download-and-extract-the-dataset" title="Link to this heading">#</a></h3>
<p>You’ll need to upload and extract your chosen dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Path to the uploaded dataset (ensure you upload the dataset file manually via Colab interface)</span>
<span class="n">zip_file</span> <span class="o">=</span> <span class="s1">&#39;/content/PlasticinWater.coco.zip&#39;</span>  <span class="c1"># OR &#39;PlasticinWaterNOAUGMENTS.coco.zip&#39;</span>

<span class="c1"># Extract the zip file</span>
<span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">zip_file</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
    <span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="s1">&#39;/content/PlasticinWater/&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-the-dataset">
<h3><span class="section-number">9.4.3. </span>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Link to this heading">#</a></h3>
<p>Next, we’ll prepare the COCO dataset for use with TensorFlow’s Object Detection API.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will need to change PlasticinWater to PlasticinWaterNOAUGMENTS if you have chosen that dataset</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">object_detection.utils</span> <span class="kn">import</span> <span class="n">config_util</span>
<span class="kn">from</span> <span class="nn">object_detection.builders</span> <span class="kn">import</span> <span class="n">model_builder</span>
<span class="kn">from</span> <span class="nn">object_detection.utils</span> <span class="kn">import</span> <span class="n">visualization_utils</span> <span class="k">as</span> <span class="n">viz_utils</span>

<span class="c1"># Define the paths to the train and validation sets</span>
<span class="n">train_images_dir</span> <span class="o">=</span> <span class="s1">&#39;/content/PlasticinWater/train/&#39;</span>
<span class="n">valid_images_dir</span> <span class="o">=</span> <span class="s1">&#39;/content/PlasticinWater/valid/&#39;</span>
<span class="n">train_annotations_file</span> <span class="o">=</span> <span class="s1">&#39;/content/PlasticinWater/train/_annotations.coco.json&#39;</span>
<span class="n">valid_annotations_file</span> <span class="o">=</span> <span class="s1">&#39;/content/PlasticinWater/valid/_annotations.coco.json&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="converting-coco-annotations-to-tensorflow-tfrecord-format">
<h3><span class="section-number">9.4.4. </span>Converting COCO Annotations to TensorFlow TFRecord Format<a class="headerlink" href="#converting-coco-annotations-to-tensorflow-tfrecord-format" title="Link to this heading">#</a></h3>
<p>To use the COCO dataset with TensorFlow, we need to convert the COCO annotations to TFRecord format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Script to convert COCO JSON to TFRecord (available from TensorFlow Object Detection API)</span>
<span class="o">!</span>python<span class="w"> </span>models/research/object_detection/dataset_tools/create_coco_tf_record.py<span class="w"> </span><span class="err">\</span>
    <span class="o">--</span><span class="n">logtostderr</span> \
    <span class="o">--</span><span class="n">train_image_dir</span><span class="o">=</span><span class="s2">&quot;$train_images_dir&quot;</span> \
    <span class="o">--</span><span class="n">val_image_dir</span><span class="o">=</span><span class="s2">&quot;$valid_images_dir&quot;</span> \
    <span class="o">--</span><span class="n">train_annotations_file</span><span class="o">=</span><span class="s2">&quot;$train_annotations_file&quot;</span> \
    <span class="o">--</span><span class="n">val_annotations_file</span><span class="o">=</span><span class="s2">&quot;$valid_annotations_file&quot;</span> \
    <span class="o">--</span><span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;/content/tfrecords/&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="transfer-learning-in-object-detection">
<h2><span class="section-number">9.5. </span>Transfer Learning in Object Detection<a class="headerlink" href="#transfer-learning-in-object-detection" title="Link to this heading">#</a></h2>
<p><strong>Transfer learning</strong> is a popular technique used in deep learning, especially in object detection tasks, where it is often difficult to train a model from scratch due to the large amount of data and time required. Instead of training a model from the ground up, <strong>transfer learning</strong> allows you to start with a pre-trained model that has already learned useful features from a large dataset (like COCO), and then fine-tune it for your specific task.</p>
<section id="why-use-transfer-learning">
<h3><span class="section-number">9.5.1. </span>Why Use Transfer Learning?<a class="headerlink" href="#why-use-transfer-learning" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Faster Convergence</strong>: Pre-trained models already have learned basic features (such as edges, textures, and shapes) from large datasets. By using a pre-trained model, your model can converge faster because it doesn’t need to learn these features from scratch.</p></li>
<li><p><strong>Smaller Dataset Requirement</strong>: Transfer learning allows you to use smaller, task-specific datasets, as the model has already learned a lot of general features from the larger dataset. For instance, in detecting plastics in water, you can start with a model pre-trained on the COCO dataset and fine-tune it on your dataset.</p></li>
<li><p><strong>Better Performance</strong>: Models trained from scratch may struggle to achieve high performance if the dataset is too small or not diverse enough. Using a pre-trained model improves performance, especially when the target task (plastic detection in water) shares similarities with the pre-training task (object detection in general).</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="walking-through-the-code">
<h3><span class="section-number">9.5.2. </span>Walking Through the Code<a class="headerlink" href="#walking-through-the-code" title="Link to this heading">#</a></h3>
<p>The following code demonstrates how to load a pre-trained detection model using the <strong>TensorFlow Object Detection API</strong> and fine-tune it for your task.</p>
</section>
<section id="code-breakdown">
<h3><span class="section-number">9.5.3. </span>Code Breakdown<a class="headerlink" href="#code-breakdown" title="Link to this heading">#</a></h3>
<section id="loading-the-pipeline-configuration-file">
<h4><span class="section-number">9.5.3.1. </span>1. <strong>Loading the Pipeline Configuration File</strong><a class="headerlink" href="#loading-the-pipeline-configuration-file" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline_config</span> <span class="o">=</span> <span class="s1">&#39;/content/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This line specifies the path to the <strong>pipeline configuration file</strong>. In this case, we are using a pre-built configuration file for the <strong>SSD ResNet50 FPN model</strong>.</p>
<ul>
<li><p><strong>SSD ResNet50 FPN</strong>: This is a model architecture that combines a <strong>Single Shot Multibox Detector (SSD)</strong> with a <strong>ResNet50</strong> backbone, enhanced with a <strong>Feature Pyramid Network (FPN)</strong>. This architecture balances speed and accuracy, making it well-suited for detecting small objects like plastics in water.</p></li>
<li><p><strong>640x640</strong>: This is the image input size used by the model, which has been pre-configured for training on the COCO dataset.</p></li>
<li><p><strong>TPU-8</strong>: This indicates that the model configuration is optimized for training on a TPU (Tensor Processing Unit) with 8 cores, although you can still use it with GPUs.</p></li>
</ul>
</li>
</ul>
</section>
<section id="loading-the-configuration-file">
<h4><span class="section-number">9.5.3.2. </span>2. <strong>Loading the Configuration File</strong><a class="headerlink" href="#loading-the-configuration-file" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">configs</span> <span class="o">=</span> <span class="n">config_util</span><span class="o">.</span><span class="n">get_configs_from_pipeline_file</span><span class="p">(</span><span class="n">pipeline_config</span><span class="p">)</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This section uses <strong><code class="docutils literal notranslate"><span class="pre">config_util.get_configs_from_pipeline_file()</span></code></strong> to read the pipeline configuration file and load it into a Python dictionary named <strong><code class="docutils literal notranslate"><span class="pre">configs</span></code></strong>.</p>
<ul>
<li><p>The <strong><code class="docutils literal notranslate"><span class="pre">configs</span></code></strong> dictionary contains different sections of the model configuration, such as the model architecture, training hyperparameters, input pipeline, and more.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">model_config</span> <span class="pre">=</span> <span class="pre">configs['model']</span></code></strong> extracts the specific configuration for the model itself (e.g., SSD with ResNet50 backbone and FPN) from the overall configuration file.</p></li>
</ul>
</section>
<section id="building-the-detection-model">
<h4><span class="section-number">9.5.3.3. </span>3. <strong>Building the Detection Model</strong><a class="headerlink" href="#building-the-detection-model" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">detection_model</span> <span class="o">=</span> <span class="n">model_builder</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">model_builder.build()</span></code></strong> is a function from the TensorFlow Object Detection API that takes the model configuration and constructs the detection model based on the specified architecture.</p>
<ul>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">model_config=model_config</span></code></strong>: Passes the loaded model configuration to the builder function to create the model as specified (SSD with ResNet50 FPN backbone).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">is_training=True</span></code></strong>: This flag sets the model in <strong>training mode</strong>, meaning the model is ready to be fine-tuned on the new dataset (i.e., plastics in water). In training mode, the model’s layers will be updated during backpropagation.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the pipeline config and build the detection model</span>
<span class="n">pipeline_config</span> <span class="o">=</span> <span class="s1">&#39;/content/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config&#39;</span>

<span class="c1"># Load the configuration file</span>
<span class="n">configs</span> <span class="o">=</span> <span class="n">config_util</span><span class="o">.</span><span class="n">get_configs_from_pipeline_file</span><span class="p">(</span><span class="n">pipeline_config</span><span class="p">)</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span>
<span class="n">detection_model</span> <span class="o">=</span> <span class="n">model_builder</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="train-the-model">
<h3><span class="section-number">9.5.4. </span>Train the Model<a class="headerlink" href="#train-the-model" title="Link to this heading">#</a></h3>
<p>This command is used to <strong>fine-tune a pre-trained model</strong> using the TensorFlow Object Detection API. Fine-tuning allows us to adapt the model to the specific task of detecting plastics in water, without needing to train from scratch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">python</span> <span class="n">models</span><span class="o">/</span><span class="n">research</span><span class="o">/</span><span class="n">object_detection</span><span class="o">/</span><span class="n">model_main_tf2</span><span class="o">.</span><span class="n">py</span> \
</pre></div>
</div>
<p>with the parameters chosen as follows:</p>
<ul class="simple">
<li><p><strong>Fine-Tuning</strong>: Starts from pre-trained weights, so the model only needs to adapt to new data.</p></li>
<li><p><strong>10000 Training Steps</strong>: Specifies the number of steps for which the model will train.</p></li>
<li><p><strong>Evaluation Samples</strong>: Evaluates the model on one validation sample after every evaluation run.</p></li>
</ul>
<p>By using this command, we fine-tune a pre-trained model to fit our dataset while saving significant time compared to training a new model from scratch.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Steps vs. Epochs</strong></p>
<p>In object detection, training is often measured in <strong>steps</strong>, while in classification tasks, we usually refer to <strong>epochs</strong>. Here’s a comparison:</p>
<ul class="simple">
<li><p><strong>Epochs (Classification Models)</strong>: An epoch refers to one complete pass over the entire training dataset. For example, training for 10 epochs means the model has seen every image in the dataset 10 times.</p></li>
<li><p><strong>Steps (Object Detection Models)</strong>: A <strong>step</strong> refers to one iteration where a batch of images is passed through the model. Training for 10,000 steps means that the model processes 10,000 batches of images, but it may not see every image in the dataset depending on the batch size and total dataset size.</p></li>
<li><p>In object detection tasks, <strong>steps</strong> are typically used due to large datasets and batch processing, whereas <strong>epochs</strong> are more common in classification tasks, where the dataset size is usually smaller.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model (example uses fine-tuning from a pre-trained model)</span>
<span class="o">!</span>python<span class="w"> </span>models/research/object_detection/model_main_tf2.py<span class="w"> </span><span class="err">\</span>
    <span class="o">--</span><span class="n">pipeline_config_path</span><span class="o">=</span><span class="err">$</span><span class="n">pipeline_config</span> \
    <span class="o">--</span><span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;/content/model/&#39;</span> \
    <span class="o">--</span><span class="n">num_train_steps</span><span class="o">=</span><span class="mi">10000</span> \
    <span class="o">--</span><span class="n">sample_1_of_n_eval_examples</span><span class="o">=</span><span class="mi">1</span> \
    <span class="o">--</span><span class="n">alsologtostderr</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate-the-model">
<h3><span class="section-number">9.5.5. </span>Evaluate the Model<a class="headerlink" href="#evaluate-the-model" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run evaluation on the validation set</span>
<span class="o">!</span>python<span class="w"> </span>models/research/object_detection/model_main_tf2.py<span class="w"> </span><span class="err">\</span>
    <span class="o">--</span><span class="n">pipeline_config_path</span><span class="o">=</span><span class="err">$</span><span class="n">pipeline_config</span> \
    <span class="o">--</span><span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;/content/model/&#39;</span> \
    <span class="o">--</span><span class="n">checkpoint_dir</span><span class="o">=</span><span class="s1">&#39;/content/model/&#39;</span> \
    <span class="o">--</span><span class="n">eval_dir</span><span class="o">=</span><span class="s1">&#39;/content/eval/&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="reflecting-on-the-evaluation-results">
<h2><span class="section-number">9.6. </span>Reflecting on the Evaluation Results<a class="headerlink" href="#reflecting-on-the-evaluation-results" title="Link to this heading">#</a></h2>
<p>Once the evaluation is complete, you will be presented with several key metrics and visualizations. <strong>Open the evaluation output</strong> and take note of the following:</p>
<section id="map-mean-average-precision">
<h3><span class="section-number">9.6.1. </span><strong>mAP (Mean Average Precision)</strong><a class="headerlink" href="#map-mean-average-precision" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Reflect on the <strong>mAP</strong> values, particularly <strong>mAP&#64;[IoU=0.50]</strong> and <strong>mAP&#64;[IoU=0.75]</strong>.</p>
<ul>
<li><p>What do these values tell you about the model’s ability to correctly predict bounding boxes?</p></li>
<li><p>Is there a large difference between <strong>mAP&#64;0.50</strong> and <strong>mAP&#64;0.75</strong>? If so, what does that say about the model’s precision at higher IoU thresholds (tighter bounding box predictions)?</p></li>
</ul>
</li>
</ul>
</section>
<section id="per-class-map">
<h3><span class="section-number">9.6.2. </span><strong>Per-Class mAP</strong><a class="headerlink" href="#per-class-map" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Look at the <strong>mAP for each class</strong>. In this case, reflect on the performance for detecting plastics in water.</p>
<ul>
<li><p>Is the model performing better for one class over others? If so, why might this be the case (e.g., more training data for certain classes, better distinguishability)?</p></li>
</ul>
</li>
</ul>
</section>
<section id="precision-and-recall">
<h3><span class="section-number">9.6.3. </span><strong>Precision and Recall</strong><a class="headerlink" href="#precision-and-recall" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Take note of the <strong>precision</strong> and <strong>recall</strong> scores provided in the evaluation output.</p>
<ul>
<li><p>Is there a balance between precision and recall? For instance, if precision is high but recall is low, the model might be too conservative and missing some detections. On the other hand, if recall is high but precision is low, the model may be making too many false positives.</p></li>
<li><p>Reflect on what this balance means for your model in the context of plastic detection in water.</p></li>
</ul>
</li>
</ul>
</section>
<section id="confusion-matrix">
<h3><span class="section-number">9.6.4. </span><strong>Confusion Matrix</strong><a class="headerlink" href="#confusion-matrix" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Review the <strong>confusion matrix</strong> to see where your model is making mistakes.</p>
<ul>
<li><p>Are there specific classes or categories that are often misclassified? Reflect on what might be causing these misclassifications (e.g., similarities between objects, poor quality images, overlapping bounding boxes).</p></li>
<li><p>Consider what steps you could take to improve these results (e.g., more data, different augmentations, improving model architecture).</p></li>
</ul>
</li>
</ul>
</section>
<section id="loss-values">
<h3><span class="section-number">9.6.5. </span><strong>Loss Values</strong><a class="headerlink" href="#loss-values" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Check the <strong>classification loss</strong> and <strong>localization loss</strong> values.</p>
<ul>
<li><p>Are these values low? If the classification loss is high, it may suggest the model is struggling to assign the correct class labels. If the localization loss is high, the model may be having trouble placing accurate bounding boxes around objects.</p></li>
<li><p>Compare these losses to the training losses. Are they significantly different? If so, reflect on whether your model may be overfitting or underfitting.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="activity-interpreting-your-results">
<h2><span class="section-number">9.7. </span>Activity: Interpreting Your Results<a class="headerlink" href="#activity-interpreting-your-results" title="Link to this heading">#</a></h2>
<p>Now that you’ve reviewed the key metrics and reflected on their meaning, summarize the evaluation results by answering the following questions:</p>
<ol class="arabic simple">
<li><p><strong>What does the mAP tell you about your model’s performance?</strong></p></li>
<li><p><strong>Is there a clear balance between precision and recall? How might this affect the model’s usability in real-world applications?</strong></p></li>
<li><p><strong>Does the confusion matrix reveal any common misclassifications? If so, what could be the cause?</strong></p></li>
<li><p><strong>How do the validation loss values compare to the training loss values? Is the model overfitting or underfitting?</strong></p></li>
</ol>
<hr class="docutils" />
<p>By reflecting on these metrics and considering what they reveal about your model, you’ll gain valuable insights into its strengths and weaknesses. Use this information to guide future improvements and adjustments to your object detection model.</p>
<section id="visualizing-results">
<h3><span class="section-number">9.7.1. </span>Visualizing Results<a class="headerlink" href="#visualizing-results" title="Link to this heading">#</a></h3>
<p>Although it is always good to get a look at how the metrics and training graphs look, another good verification of how your model is doing is looking at it perform <strong>inferences</strong> on unseen test images. Visualizing the predictions can help you identify patterns that may not be clear from the metrics alone. For example, you can spot cases where the model consistently misplaces bounding boxes, misses small objects, or misclassifies certain items.This can be particularly useful in object detection tasks where the model’s success depends not only on correctly identifying the presence of objects but also on accurately placing bounding boxes around them. By visualizing results on test images, you can quickly assess whether the model is over-detecting objects (too many false positives) or missing important detections (false negatives). You may also observe patterns in failure cases, such as objects partially out of frame or overlapping plastics in the water. Moreover, visualizing results helps you evaluate the model’s <strong>confidence scores</strong>, which tell you how certain the model is about its predictions. For example, a prediction with a high confidence score but visibly incorrect bounding boxes or classifications would indicate that the model is overconfident in that instance, suggesting a need for additional tuning or more diverse training data. This process adds a qualitative aspect to the quantitative evaluation, allowing you to better judge how your model performs in real-world scenarios. It’s an essential step for fine-tuning and deciding how to improve the model, whether it involves tweaking hyperparameters, introducing more data, or applying different augmentations.</p>
<p>import matplotlib.pyplot as plt
import cv2
import os
from object_detection.utils import visualization_utils as viz_utils</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="get-the-first-image-from-the-test-directory">
<h1><span class="section-number">10. </span>Get the first image from the test directory<a class="headerlink" href="#get-the-first-image-from-the-test-directory" title="Link to this heading">#</a></h1>
<p>test_images_dir = ‘/content/PlasticinWater/test/’
test_image_files = sorted(os.listdir(test_images_dir))
first_image_path = os.path.join(test_images_dir, test_image_files[0])</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-the-first-test-image">
<h1><span class="section-number">11. </span>Load the first test image<a class="headerlink" href="#load-the-first-test-image" title="Link to this heading">#</a></h1>
<p>image_np = cv2.imread(first_image_path)</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="run-the-detection-model-on-the-image">
<h1><span class="section-number">12. </span>Run the detection model on the image<a class="headerlink" href="#run-the-detection-model-on-the-image" title="Link to this heading">#</a></h1>
<p>detections = detection_model(image_np)</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="visualize-the-detection-results-on-the-image">
<h1><span class="section-number">13. </span>Visualize the detection results on the image<a class="headerlink" href="#visualize-the-detection-results-on-the-image" title="Link to this heading">#</a></h1>
<p>viz_utils.visualize_boxes_and_labels_on_image_array(
image_np,
detections[‘detection_boxes’],
detections[‘detection_classes’],
detections[‘detection_scores’],
category_index,
use_normalized_coordinates=True,
min_score_thresh=0.5
)</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="display-the-result">
<h1><span class="section-number">14. </span>Display the result<a class="headerlink" href="#display-the-result" title="Link to this heading">#</a></h1>
<p>plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))
plt.axis(‘off’)
plt.show()</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="LA5.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Image Classification with Keras</p>
      </div>
    </a>
    <a class="right-next"
       href="LA7.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">15. </span>Instance Segmentation with Detectron2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">9. LA6: Object Detection with TensorFlow API</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">9.1. Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#downloading-the-dataset">9.2. Downloading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-coco-format">9.3. Understanding the COCO Format</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#file-structure">9.3.1. File Structure:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coco-json-structure">9.3.2. COCO JSON Structure:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-environment">9.4. Preparing the Environment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#install-tensorflow-object-detection-api">9.4.1. Install TensorFlow Object Detection API</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-and-extract-the-dataset">9.4.2. Download and Extract the Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">9.4.3. Loading the Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-coco-annotations-to-tensorflow-tfrecord-format">9.4.4. Converting COCO Annotations to TensorFlow TFRecord Format</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-in-object-detection">9.5. Transfer Learning in Object Detection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-transfer-learning">9.5.1. Why Use Transfer Learning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#walking-through-the-code">9.5.2. Walking Through the Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-breakdown">9.5.3. Code Breakdown</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-pipeline-configuration-file">9.5.3.1. 1. <strong>Loading the Pipeline Configuration File</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-configuration-file">9.5.3.2. 2. <strong>Loading the Configuration File</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-detection-model">9.5.3.3. 3. <strong>Building the Detection Model</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-model">9.5.4. Train the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-model">9.5.5. Evaluate the Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflecting-on-the-evaluation-results">9.6. Reflecting on the Evaluation Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-mean-average-precision">9.6.1. <strong>mAP (Mean Average Precision)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-class-map">9.6.2. <strong>Per-Class mAP</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-and-recall">9.6.3. <strong>Precision and Recall</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">9.6.4. <strong>Confusion Matrix</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-values">9.6.5. <strong>Loss Values</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-interpreting-your-results">9.7. Activity: Interpreting Your Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-results">9.7.1. Visualizing Results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#get-the-first-image-from-the-test-directory">10. Get the first image from the test directory</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-first-test-image">11. Load the first test image</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#run-the-detection-model-on-the-image">12. Run the detection model on the image</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-the-detection-results-on-the-image">13. Visualize the detection results on the image</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#display-the-result">14. Display the result</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Atticus Carter
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>