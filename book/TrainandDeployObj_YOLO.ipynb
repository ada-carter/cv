{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "976c71ad",
      "metadata": {
        "id": "976c71ad"
      },
      "source": [
        "\n",
        "# Training and Deploying Object Detection with YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R4Ry19D5iXZc",
      "metadata": {
        "id": "R4Ry19D5iXZc"
      },
      "source": [
        "### Overview\n",
        "In this lesson, we will train an Object Detection model using YOLOv11. You'll be able to choose specific augmentations, batch size, resolution, and other parameters based on your system's capabilities and runtime. The dataset is already provided in YOLO format and will be used to train and evaluate the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1sSo5Q2giXZd",
      "metadata": {
        "id": "1sSo5Q2giXZd"
      },
      "source": [
        "### Learning Objectives\n",
        "By the end of this section, you will:\n",
        "- Understand the YOLO format and how to train a custom object detection model using YOLOv11.\n",
        "- Experiment with different augmentations and hyperparameters for object detection.\n",
        "- Evaluate the model's performance and visualize the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CnbapsstiXZd",
      "metadata": {
        "id": "CnbapsstiXZd"
      },
      "source": [
        "### Downloading the Dataset\n",
        "The dataset for this lesson is already formatted in YOLO format. You can load it directly for training and evaluation. Ensure you have the dataset uploaded before proceeding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R1qwZw0giXZd",
      "metadata": {
        "id": "R1qwZw0giXZd"
      },
      "source": [
        "### Preparing the Environment\n",
        "Let's first install the required libraries and set up the environment to train our YOLOv11 model. **Crucially** Make sure that you are in a GPU runtime by running the cell below. It should output the GPU currently connected to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbMKUGG0rtd4",
      "metadata": {
        "id": "hbMKUGG0rtd4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C-LGjU7xiXZd",
      "metadata": {
        "collapsed": true,
        "id": "C-LGjU7xiXZd"
      },
      "outputs": [],
      "source": [
        "# Install the required dependencies\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Lv9QQWGiXZe",
      "metadata": {
        "id": "0Lv9QQWGiXZe"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "import json\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nRNiXsuUiXZe",
      "metadata": {
        "id": "nRNiXsuUiXZe"
      },
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "To download the dataset used in this tutorial, visit [this link](https://tinyurl.com/462cplastic). Make sure to select \"YOLOV11\" as the format and choose the zip download option. Once the zip file is downloaded to your computer, upload it to your Colab runtime environment. You can do this by clicking the folder icon on the left sidebar and uploading the file there.\n",
        "\n",
        "If you are using a different dataset or format, ensure it is structured in the YOLO format. For this tutorial, we assume that the dataset is organized into `train/`, `val/`, and `test/` directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z3g9MZGFiXZe",
      "metadata": {
        "id": "Z3g9MZGFiXZe"
      },
      "outputs": [],
      "source": [
        "# Set paths to the dataset\n",
        "# Replace with the path to your zip folder, which can be found by rightclicking\n",
        "# on it in the file browser.\n",
        "\n",
        "zip_file_path = '/content/ClassPlastics.v1i.yolov11.zip'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Dataset')\n",
        "\n",
        "# Set the dataset path\n",
        "dataset_path = '/content/Dataset'\n",
        "\n",
        "# Verify the dataset path\n",
        "print(f\"Dataset path is set to: {dataset_path}\")\n",
        "print(f\"Files in dataset path: {os.listdir(dataset_path)}\")\n",
        "\n",
        "# Set train and val paths\n",
        "train_path = os.path.join(dataset_path, 'train/')\n",
        "val_path = os.path.join(dataset_path, 'val/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MRMt5NeaiXZf",
      "metadata": {
        "id": "MRMt5NeaiXZf"
      },
      "source": [
        "\n",
        "### Initializing TensorBoard Before Training\n",
        "\n",
        "TensorBoard is a powerful visualization tool that provides real-time insights into your model's training process. By initializing TensorBoard before training, you can monitor key metrics such as loss, accuracy, and learning rates, allowing for timely adjustments and improved model performance. This proactive monitoring helps in identifying issues like overfitting or underfitting early in the training process. Be sure to click the refresh button in the top right of the Tensorboard often!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KA2c44gSpNWH",
      "metadata": {
        "id": "KA2c44gSpNWH"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/runs/detect/train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vQod-jpkiXZf",
      "metadata": {
        "id": "vQod-jpkiXZf"
      },
      "source": [
        "## Training the YOLOv11 Model\n",
        "\n",
        "Key Training Parameters: imgsz, batch, and epochs\n",
        "\n",
        "imgsz (Image Size): This parameter defines the target size to which all training images are resized. A standard value is 640 pixels, but adjusting this can impact model accuracy and computational load. Larger sizes may improve accuracy but require more resources, while smaller sizes can speed up training at the cost of precision.\n",
        "\n",
        "\n",
        "batch (Batch Size): This determines the number of images processed simultaneously during training. Setting an appropriate batch size is essential; too large can lead to memory issues, while too small may result in unstable training. YOLOv11 offers flexibility, allowing you to set a specific integer (e.g., batch=16), use auto mode for 60% GPU memory utilization (batch=-1), or specify a utilization fraction (batch=0.70).\n",
        "\n",
        "\n",
        "epochs: This defines the number of complete passes through the training dataset. Choosing the right number of epochs is vital; too few may lead to underfitting, while too many can cause overfitting. Monitoring performance metrics during training can help determine the optimal number of epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Grz_vz2iXZf",
      "metadata": {
        "id": "1Grz_vz2iXZf"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv11 model (pretrained on COCO dataset)\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "# Path to the dataset configuration YAML file\n",
        "dataset_config = '/content/Dataset/data.yaml'  # Path to the YAML file\n",
        "\n",
        "# Train the model\n",
        "results = model.train(\n",
        "    data=dataset_config,  # Path to the YAML file\n",
        "    epochs=100,\n",
        "    batch=64,  # Set a valid batch size (adjust as needed)\n",
        "    imgsz=640,  # Image size for training\n",
        "    plots=True,\n",
        "    patience=50\n",
        ")\n",
        "\n",
        "# Optionally, you can print the results after training to inspect\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9R6QqG-dtHZV",
      "metadata": {
        "id": "9R6QqG-dtHZV"
      },
      "source": [
        "## Loss\n",
        "\n",
        "As you are waiting for your model to train, take note of the the loss values. In YOLOv11, the loss function comprises three primary components: box loss, class loss, and Distribution Focal Loss (DFL). Each plays a distinct role in training the model effectively.\n",
        "\n",
        "### Box Loss\n",
        "\n",
        "Box loss is responsible for optimizing the localization accuracy of predicted bounding boxes. It measures the discrepancy between the predicted boxes and the ground truth annotations. YOLOv11 employs the Complete Intersection over Union (CIoU) loss for this purpose, which considers:\n",
        "\n",
        "Overlap Area: The intersection over union between the predicted and ground truth boxes.\n",
        "Distance Between Centers: How far apart the centers of the two boxes are.\n",
        "Aspect Ratio Consistency: Differences in the width and height ratios of the boxes.\n",
        "By integrating these factors, CIoU provides a comprehensive measure for bounding box regression, leading to more precise localization.\n",
        "\n",
        "### Class Loss\n",
        "\n",
        "Class loss ensures that the model accurately classifies detected objects into their respective categories. It is typically calculated using Cross-Entropy Loss, which evaluates the difference between the predicted class probabilities and the actual class labels. Minimizing this loss helps the model improve its classification performance.\n",
        "\n",
        "### Distribution Focal Loss (DFL)\n",
        "\n",
        "DFL is designed to enhance the model's ability to distinguish between objects that are similar or challenging to differentiate. It focuses on refining the bounding box predictions by emphasizing harder-to-classify examples, improving the model's discriminative power. This is particularly beneficial in scenarios with class imbalance or when dealing with small or ambiguous objects.\n",
        "\n",
        "Each of these loss components contributes to the overall training objective by addressing different aspects of the object detection task: localization, classification, and the handling of difficult examples. Balancing these losses appropriately is crucial for achieving optimal model performance.\n",
        "\n",
        "## Fitting\n",
        "\n",
        "Monitoring loss metrics is crucial for assessing model performance and identifying signs of overfitting or underfitting. In YOLOv11, consistently decreasing box loss, class loss, and Distribution Focal Loss (DFL) during training indicates effective learning. However, if these loss metrics stagnate—showing no significant improvement over successive epochs—it may suggest that the model has reached its optimal capacity or is encountering issues such as overfitting or underfitting.\n",
        "\n",
        "### Overfitting\n",
        "\n",
        "Overfitting occurs when the model performs well on training data but poorly on validation data, indicating it has memorized the training examples rather than generalizing from them. This is often observed when training loss continues to decrease while validation loss starts to increase. To mitigate overfitting, techniques such as early stopping can be employed. In YOLOv11, you can set the patience parameter in your training configuration to specify the number of epochs to wait for an improvement in validation metrics before stopping training. For example, setting patience=5 will halt training if there's no improvement in validation metrics for five consecutive epochs.\n",
        "\n",
        "### Underfitting\n",
        "\n",
        "Underfitting is characterized by poor performance on both training and validation datasets, suggesting the model is too simplistic to capture the underlying patterns in the data. This can be identified when both training and validation losses are high and show minimal improvement. To address underfitting, consider increasing the model's complexity, providing more training data, or adjusting hyperparameters to better capture the data's intricacies.\n",
        "\n",
        "By closely monitoring these loss metrics and implementing strategies like early stopping with an appropriate patience parameter, you can ensure efficient training, prevent overfitting, and achieve optimal model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2x3mg1IuiXZf",
      "metadata": {
        "id": "2x3mg1IuiXZf"
      },
      "source": [
        "### Evaluating the Model\n",
        "After training, we will evaluate the model performance using validation data and calculated metrics such as mean Average Precision (mAP). YOLOv11 will perform evaluation automatically after running its training mode, however if you stopped early or have other reasons to run validation after a model is trained, you can do so using the val mode\n",
        "\n",
        "```\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model_path = '/content/runs/detect/train/weights/best.pt'\n",
        "model = YOLO(model_path)\n",
        "test_images_dir = '/content/Dataset/test/images'\n",
        "results = model.predict(source=test_images_dir, save=True, save_txt=True)\n",
        "results\n",
        "\n",
        "```\n",
        "For now, we will assume that your model was trained to its set number of epochs, so we wil be displaying the graphs directly from its train directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HxXu9EeQwk-4",
      "metadata": {
        "id": "HxXu9EeQwk-4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# Set the base directory\n",
        "base_dir = \"/content/runs/detect/train/\"\n",
        "\n",
        "# List of filenames to display\n",
        "filenames = [\n",
        "    \"labels.jpg\",\n",
        "    \"F1_curve.png\",\n",
        "    \"PR_curve.png\",\n",
        "    \"P_curve.png\",\n",
        "    \"R_curve.png\",\n",
        "    \"confusion_matrix.png\",\n",
        "    \"confusion_matrix_normalized.png\"\n",
        "]\n",
        "\n",
        "# Display each image\n",
        "for filename in filenames:\n",
        "    image_path = os.path.join(base_dir, filename)\n",
        "    display(Image(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G9Ed7XApzDmP",
      "metadata": {
        "id": "G9Ed7XApzDmP"
      },
      "source": [
        "## Inference\n",
        "\n",
        "After training your model and evaluating its performance, the next step is to run inference on a video to assess its real-world applicability. Within this code cell, you can adjust two inference parameters: confidence threshold (conf) and Intersection over Union threshold (iou).\n",
        "\n",
        "### Confidence Threshold (conf)\n",
        "(default: 0.25) This parameter sets the minimum confidence level for detections. Objects detected with a confidence score below this threshold will be disregarded. Adjusting this value can help reduce false positives.\n",
        "\n",
        "### Intersection over Union (IoU)\n",
        "iou (default: 0.7): This parameter defines the IoU threshold for Non-Maximum Suppression (NMS). Lower values result in fewer detections by eliminating overlapping boxes, which is useful for reducing duplicates.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1S-O4Igq-NA3",
      "metadata": {
        "id": "1S-O4Igq-NA3"
      },
      "source": [
        "Use the following code block to download a video to test your new model on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBLAmS9_-DlP",
      "metadata": {
        "id": "pBLAmS9_-DlP"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/datasets/OceanCV/PlasticTank_Video/resolve/main/tankvid.mp4?download=true -O tankvid.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2EflLwrcypx2",
      "metadata": {
        "id": "2EflLwrcypx2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model_path = '/content/runs/detect/train/weights/best.pt'\n",
        "model = YOLO(model_path)\n",
        "\n",
        "video_path = 'tankvid.mp4'\n",
        "\n",
        "results = model.predict(source=video_path, conf=0.25, iou=0.7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DVpUtJeMiXZg",
      "metadata": {
        "id": "DVpUtJeMiXZg"
      },
      "source": [
        "### Reflecting on Results\n",
        "Now that you've trained and evaluated your model, reflect on the following questions:\n",
        "\n",
        "- How might the parameters affect the model's performance and how would you design an experiment to test the best params for your usecase?\n",
        "- Were there any significant differences in the val metrics for different classes, why?\n",
        "- What visual observations can you make from the test results?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
