
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. Image Classification with Keras &#8212; Computer Vision Across Oceanography</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/LA5';</script>
    <link rel="canonical" href="https://atticus-carter.github.io/cv/book/LA5.html" />
    <link rel="icon" href="../_static/fav.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. LA6: Object Detection with TensorFlow API" href="LA6.html" />
    <link rel="prev" title="7. Understanding CV Metrics and Graphs" href="LA4.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Computer Vision Across Oceanography - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Computer Vision Across Oceanography - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 1 - Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="LA1.html">1. Introduction to Ocean Image Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA1.5.html">2. Marine Imaging Workshop Survey Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA2.html">3. Image Annotation</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA2.5.html">4. Image Manipulation in Python with PIL and OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA2.75.html">5. Creating and Augmenting Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA3.html">6. The Math Behind Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA4.html">7. Understanding CV Metrics and Graphs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 2 - Computer Vision Development</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Image Classification with Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA6.html">9. LA6: Object Detection with TensorFlow API</a></li>





<li class="toctree-l1"><a class="reference internal" href="LA7.html">15. Instance Segmentation with Detectron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA8.html">16. Keypoint Detection with MediaPipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA9.html">17. Object and Multi-Object Tracking with TensorFlow and OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA10.html">18. Image Super-Resolution and Enhancement with SRGAN in TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA11.html">19. Self-Supervised Learning for Image Classification with SimCLR in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA12.html">20. Action Recognition and Event Detection in Videos using I3D Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA13.html">21. Anomaly Detection in Images and Videos using Autoencoders and TensorFlow</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 3 - Synthesis Project</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="LA14.html">22. Dataset Preparation and Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA15.html">23. Model Selection and Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA16.html">24. Training and Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA17.html">25. Data Visualization and Figure Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA18.html">26. Usecase Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="LA19.html">27. Tying it all together</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/atticus-carter/cv/master?urlpath=tree/book/LA5.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/atticus-carter/cv/blob/master/book/LA5.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/atticus-carter/cv" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/atticus-carter/cv/edit/main/book/LA5.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/atticus-carter/cv/issues/new?title=Issue%20on%20page%20%2Fbook/LA5.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/book/LA5.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Image Classification with Keras</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">8.1. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">8.1.1. Learning Objectives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection-vs-image-classification">8.1.2. Object Detection vs. Image Classification</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries-overview">8.2. Libraries Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-description">8.3. Dataset Description</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-dataset-parameters">8.4. Understanding Dataset Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">8.5. Loading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-data">8.6. Visualizing the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-dataset-structure">8.7. Verifying Dataset Structure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-tensors">8.7.1. What Are Tensors?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-breakdown">8.7.2. Code Breakdown:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose">8.7.3. Purpose:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-basic-keras-model">8.8. A Basic Keras Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-preparation-for-efficient-processing">8.8.1. Dataset Preparation for Efficient Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-layer">8.8.2. Normalization Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-structure-sequential-api">8.8.3. Model Structure: Sequential API</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">8.8.3.1. Input Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-convolutional-block">8.8.3.2. First Convolutional Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#second-convolutional-block">8.8.3.3. Second Convolutional Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#third-convolutional-block">8.8.3.4. Third Convolutional Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-and-dense-layers">8.8.3.5. Flattening and Dense Layers</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-importance">8.8.4. Layer Importance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compiling-the-model">8.9. Compiling the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-adam">8.9.1. Optimizer: Adam</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-sparse-categorical-crossentropy">8.9.2. Loss Function: Sparse Categorical Crossentropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-accuracy">8.9.3. Metrics: Accuracy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-summary">8.10. Model Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model">8.11. Training the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-training-results">8.12. Visualizing Training Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions-on-a-new-image">8.13. Making Predictions on a New Image</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="image-classification-with-keras">
<h1><span class="section-number">8. </span>Image Classification with Keras<a class="headerlink" href="#image-classification-with-keras" title="Link to this heading">#</a></h1>
<section id="overview">
<h2><span class="section-number">8.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>In this lesson, we will explore image classification using a <strong>Convolutional Neural Network (CNN)</strong> in <strong>Keras</strong> with <strong>TensorFlow</strong>. Keras is a high-level API built on top of TensorFlow, designed to simplify the process of building and training neural networks. TensorFlow, on the other hand, is a comprehensive open-source machine learning framework that provides powerful tools for building and deploying deep learning models, including neural networks.</p>
<p>We will use a dataset of benthic animals to classify images of <strong>crabs</strong> and <strong>rockfish</strong>, which were captured in 2011 by the <strong>ROV ROPOS</strong> at <strong>Southern Hydrate Ridge</strong>, as part of the <strong>Ocean Observatories Initiative Regional Cabled Array</strong> operated by the University of Washington and funded by the National Science Foundation (NSF).</p>
<section id="learning-objectives">
<h3><span class="section-number">8.1.1. </span>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h3>
<p>By the end of this section, you will:</p>
<ul class="simple">
<li><p>Understand how to load, preprocess, and split image datasets for training a neural network.</p></li>
<li><p>Develop, compile, and train a Convolutional Neural Network (CNN) model for image classification using Keras.</p></li>
<li><p>Start to visualize and interpret training and validation accuracy and loss graphs, and evaluate model performance.</p></li>
<li><p>Make predictions on new, unseen images and assess the model’s confidence in classifying them.</p></li>
</ul>
</section>
<section id="object-detection-vs-image-classification">
<h3><span class="section-number">8.1.2. </span>Object Detection vs. Image Classification<a class="headerlink" href="#object-detection-vs-image-classification" title="Link to this heading">#</a></h3>
<p>There are two common tasks in computer vision: <strong>object detection</strong> and <strong>image classification</strong>.</p>
<ul class="simple">
<li><p><strong>Image classification</strong> involves categorizing an entire image into a single class. In this case, we are classifying each image as either a “crab” or a “rockfish.”</p></li>
<li><p><strong>Object detection</strong>, on the other hand, is more complex. It involves not only identifying the objects present in an image but also determining their exact location within the image. This requires the use of annotations (usually separate files) to provide bounding boxes around each object.</p></li>
</ul>
<p>In this lesson, we focus on <strong>image classification</strong>. The images used in this dataset are not annotated with bounding boxes because they are assumed to contain only one of the two possible classes—either “crab” or “rockfish.” This simplifies the problem, allowing us to rely on the image file names and folder structure to determine the class labels. No separate annotation files are necessary.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following activity requires a small dataset download, you can download it here: - <a class="reference external" href="https://drive.google.com/file/d/1aX3nUPtPWp3ScgGU1q4JPkZRLAyuo8ax/view?usp=sharing">SHRCrabsandFishClassification.zip</a></p>
<p>Alternatively, you can use a dataset modified in the previous lesson!</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="libraries-overview">
<h2><span class="section-number">8.2. </span>Libraries Overview<a class="headerlink" href="#libraries-overview" title="Link to this heading">#</a></h2>
<p>Here’s a quick overview of the libraries used in this activity:</p>
<ul class="simple">
<li><p><strong>matplotlib.pyplot</strong>: A plotting library used to visualize data. In this lesson, it helps display images and graphs of model performance.</p></li>
<li><p><strong>numpy</strong>: A fundamental library for numerical operations in Python, often used for handling arrays and matrices.</p></li>
<li><p><strong>PIL (Python Imaging Library)</strong>: Provides functionality for opening, manipulating, and saving image files. In this lesson, it’s used to display sample images.</p></li>
<li><p><strong>OpenCV (cv2)</strong>: Provides advanced image augmentation</p></li>
<li><p><strong>tensorflow</strong>: A popular machine learning framework used for building and training models. We use TensorFlow’s high-level Keras API to create and train the Convolutional Neural Network (CNN) in this lesson.</p></li>
<li><p><strong>random</strong>: A standard Python module used here to randomly select and display images from the dataset.</p></li>
<li><p><strong>pathlib</strong>: Used for handling and manipulating file system paths in an easy-to-use manner.</p></li>
<li><p><strong>zipfile</strong>: A Python library for handling <code class="docutils literal notranslate"><span class="pre">.zip</span></code> files. Here, it’s used to extract the dataset.</p></li>
<li><p><strong>tensorflow.keras</strong>: A high-level neural networks API, included with TensorFlow, used to create layers and models for machine learning.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing Required Libraries</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">PIL</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataset-description">
<h2><span class="section-number">8.3. </span>Dataset Description<a class="headerlink" href="#dataset-description" title="Link to this heading">#</a></h2>
<p>This tutorial uses a dataset of <strong>800 photos</strong> of benthic animals, consisting of two classes:</p>
<ul class="simple">
<li><p><strong>crab</strong></p></li>
<li><p><strong>rockfish</strong></p></li>
</ul>
<p>These images are organized into two subdirectories within the dataset folder:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">new_benthic_photo</span><span class="o">/</span>
  <span class="n">crab</span><span class="o">/</span>
  <span class="n">rockfish</span><span class="o">/</span>
</pre></div>
</div>
</div>
</div>
<p>Now lets set up our downloaded dataset, to do this import your unextracted zip file into colab under the files section on the left side of your screen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">&quot;/content/SHRCrabsandFishClassification.zip&quot;</span>

<span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
    <span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="s2">&quot;/content/SHRCrabsandFishClassification&quot;</span><span class="p">)</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/content/SHRCrabsandFishClassification&quot;</span><span class="p">)</span>

<span class="n">image_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">data_dir</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;*/*.png&#39;</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Total number of images: </span><span class="si">{</span><span class="n">image_count</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">crabs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data_dir</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;crab/*&#39;</span><span class="p">))</span>
<span class="n">rockfish</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data_dir</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;rockfish/*&#39;</span><span class="p">))</span>

<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">crabs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">crabs</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">rockfish</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">rockfish</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Data Directories</span>

<span class="n">crab_dir</span> <span class="o">=</span> <span class="n">data_dir</span> <span class="o">/</span> <span class="s1">&#39;crab&#39;</span>
<span class="n">rockfish_dir</span> <span class="o">=</span> <span class="n">data_dir</span> <span class="o">/</span> <span class="s1">&#39;rockfish&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># List all images in the &#39;crab&#39; folder</span>
<span class="n">crab_images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">crab_dir</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">))</span>  
<span class="n">random_crab</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">crab_images</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Randomly select from all crab images</span>

<span class="c1"># Load and display the crab image using OpenCV and Matplotlib</span>
<span class="n">crab_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">crab_images</span><span class="p">[</span><span class="n">random_crab</span><span class="p">]))</span>
<span class="n">crab_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">crab_image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>  <span class="c1"># Convert BGR to RGB for correct color display</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">crab_image</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Crab Image&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># List all images in the &#39;rockfish&#39; folder</span>
<span class="n">rockfish_images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">rockfish_dir</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">))</span>  
<span class="n">random_rockfish</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rockfish_images</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Randomly select from all rockfish images</span>

<span class="c1"># Load and display the rockfish image using OpenCV and Matplotlib</span>
<span class="n">rockfish_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">rockfish_images</span><span class="p">[</span><span class="n">random_rockfish</span><span class="p">]))</span>
<span class="n">rockfish_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">rockfish_image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>  <span class="c1"># Convert BGR to RGB for correct color display</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">rockfish_image</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Rockfish Image&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By defining the data directories for each class (crab and rockfish), we make it easier to access and manage the images belonging to each category. This allows us to quickly refer to the directories when performing operations like random sampling, image preprocessing, or displaying specific examples from each class. This is a bit overkill with only 2 classes, but its good practice because it ensures that our code is organized and can efficiently work with large datasets where images are separated into folders based on their classes.</p>
</div>
</section>
<section id="understanding-dataset-parameters">
<h2><span class="section-number">8.4. </span>Understanding Dataset Parameters<a class="headerlink" href="#understanding-dataset-parameters" title="Link to this heading">#</a></h2>
<p>Here, we set key parameters for handling the dataset:</p>
<ul class="simple">
<li><p><strong>batch_size = 32</strong>: This defines how many images will be processed in one pass through the model. Using a batch size of 32 means that during training, the model will look at 32 images before updating its internal parameters. A batch size of 32 is commonly used for balancing memory efficiency and training speed.</p></li>
<li><p><strong>img_height = 180</strong> and <strong>img_width = 180</strong>: These define the dimensions to which each image will be resized. By resizing all images to a uniform height and width of 180x180 pixels, we ensure consistency in input size, which is required for neural networks. Although resizing reduces detail, it also speeds up computation and simplifies model training.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set Parameters for the Dataset</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">img_height</span> <span class="o">=</span> <span class="mi">180</span>
<span class="n">img_width</span> <span class="o">=</span> <span class="mi">180</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-the-dataset">
<h2><span class="section-number">8.5. </span>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Link to this heading">#</a></h2>
<p>We use the <code class="docutils literal notranslate"><span class="pre">tf.keras.utils.image_dataset_from_directory</span></code> function to load and preprocess the dataset. Here’s what each argument does:</p>
<ul class="simple">
<li><p><strong>data_dir</strong>: This points to the directory where the dataset is stored (the root folder containing the <code class="docutils literal notranslate"><span class="pre">crab</span></code> and <code class="docutils literal notranslate"><span class="pre">rockfish</span></code> subdirectories).</p></li>
<li><p><strong>validation_split = 0.2</strong>: This splits the dataset into training and validation sets. In this case, 80% of the data will be used for training, and 20% for validation.</p></li>
<li><p><strong>subset</strong>: We specify whether we’re creating the <strong>training</strong> or <strong>validation</strong> dataset. Setting <code class="docutils literal notranslate"><span class="pre">subset=&quot;training&quot;</span></code> creates the training set, and <code class="docutils literal notranslate"><span class="pre">subset=&quot;validation&quot;</span></code> creates the validation set.</p></li>
<li><p><strong>seed = 123</strong>: A seed value ensures that the dataset split is reproducible, meaning that the same data will always be allocated to the training and validation sets when the code is rerun.</p></li>
<li><p><strong>image_size = (img_height, img_width)</strong>: This resizes each image to the predefined size of <code class="docutils literal notranslate"><span class="pre">180x180</span></code> pixels, ensuring all images are consistent when input to the model.</p></li>
<li><p><strong>batch_size = 32</strong>: This defines the number of images processed in each batch, ensuring efficient training and memory usage.</p></li>
</ul>
<p>This setup loads the dataset in a format that’s ready for model training while automatically handling preprocessing like image resizing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading the Dataset</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
  <span class="n">data_dir</span><span class="p">,</span>
  <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
  <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">,</span>
  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
  <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">val_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
  <span class="n">data_dir</span><span class="p">,</span>
  <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
  <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span>
  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
  <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display Class Names</span>

<span class="n">class_names</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">class_names</span>
<span class="nb">print</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-the-data">
<h2><span class="section-number">8.6. </span>Visualizing the Data<a class="headerlink" href="#visualizing-the-data" title="Link to this heading">#</a></h2>
<p>This block of code is used to visualize a sample of images from the training dataset. Here’s what each part does:</p>
<ul class="simple">
<li><p><strong>plt.figure(figsize=(10, 10))</strong>: This sets up the figure size for displaying the images. The size <code class="docutils literal notranslate"><span class="pre">(10,</span> <span class="pre">10)</span></code> ensures a large enough grid to comfortably view multiple images.</p></li>
<li><p><strong>train_ds.take(1)</strong>: This grabs a single batch of images and labels from the training dataset. Since the batch size is set to 32, this will retrieve 32 images and their corresponding labels, but we’re only displaying 9 of them.</p></li>
<li><p><strong>for i in range(9)</strong>: This loop goes through the first 9 images in the batch and plots them.</p></li>
<li><p><strong>ax = plt.subplot(3, 3, i + 1)</strong>: This creates a <code class="docutils literal notranslate"><span class="pre">3x3</span></code> grid of subplots to display 9 images.</p></li>
<li><p><strong>plt.imshow(images[i].numpy().astype(“uint8”))</strong>: This converts each image tensor into a NumPy array and displays it as an image.</p></li>
<li><p><strong>plt.title(class_names[labels[i]])</strong>: This adds the class name (either “crab” or “rockfish”) as the title above each image based on the label associated with the image.</p></li>
<li><p><strong>plt.axis(“off”)</strong>: This hides the axes for a cleaner visualization of the images.</p></li>
</ul>
<p>The purpose of this block is to quickly visualize how the dataset looks, allowing us to verify that the images and their labels are being loaded correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the Data</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="verifying-dataset-structure">
<h2><span class="section-number">8.7. </span>Verifying Dataset Structure<a class="headerlink" href="#verifying-dataset-structure" title="Link to this heading">#</a></h2>
<p>In this block of code, we inspect the structure of the dataset by printing the shape of a batch of images and their corresponding labels. Before diving into the code, let’s briefly discuss <strong>tensors</strong>.</p>
<section id="what-are-tensors">
<h3><span class="section-number">8.7.1. </span>What Are Tensors?<a class="headerlink" href="#what-are-tensors" title="Link to this heading">#</a></h3>
<p>A <strong>tensor</strong> is a multi-dimensional array that generalizes matrices to higher dimensions. Tensors are the basic data structure in deep learning and are used to represent inputs, weights, and outputs of models. In our case, each image is represented as a 3D tensor (height, width, color channels), and the dataset is organized into batches of these tensors. The labels are 1D tensors representing the class of each image.</p>
</section>
<section id="code-breakdown">
<h3><span class="section-number">8.7.2. </span>Code Breakdown:<a class="headerlink" href="#code-breakdown" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>for image_batch, labels_batch in train_ds</strong>: This loop retrieves a single batch of images and labels from the training dataset. Since our <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is 32, the batch will contain 32 images and their corresponding labels.</p></li>
<li><p><strong>print(image_batch.shape)</strong>: This prints the shape of the <code class="docutils literal notranslate"><span class="pre">image_batch</span></code>, which should be <code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">180,</span> <span class="pre">180,</span> <span class="pre">3)</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">32</span></code>: The batch size (32 images).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">180,</span> <span class="pre">180</span></code>: The dimensions of each image, resized to 180x180 pixels.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">3</span></code>: The number of color channels (RGB).</p></li>
</ul>
</li>
<li><p><strong>print(labels_batch.shape)</strong>: This prints the shape of the <code class="docutils literal notranslate"><span class="pre">labels_batch</span></code>, which should be <code class="docutils literal notranslate"><span class="pre">(32,)</span></code> because there are 32 labels, one for each image in the batch.</p></li>
<li><p><strong>break</strong>: This ensures that the loop runs only once, as we only need to check the structure of one batch.</p></li>
</ul>
</section>
<section id="purpose">
<h3><span class="section-number">8.7.3. </span>Purpose:<a class="headerlink" href="#purpose" title="Link to this heading">#</a></h3>
<p>This step is important to confirm that the dataset is loaded correctly and the images and labels are properly batched and shaped, making them ready for input into the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Verify Dataset Structure</span>

<span class="k">for</span> <span class="n">image_batch</span><span class="p">,</span> <span class="n">labels_batch</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">image_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">labels_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="a-basic-keras-model">
<h2><span class="section-number">8.8. </span>A Basic Keras Model<a class="headerlink" href="#a-basic-keras-model" title="Link to this heading">#</a></h2>
<p>In this section, we define the <strong>Convolutional Neural Network (CNN)</strong> architecture for image classification. Unlike the previous lesson where we used a single layer with an <strong>edge detection kernel</strong>, here we employ a deeper network with multiple layers. Each layer plays a crucial role in feature extraction and learning from the dataset. Let’s go through each part of this code in detail.</p>
<section id="dataset-preparation-for-efficient-processing">
<h3><span class="section-number">8.8.1. </span>Dataset Preparation for Efficient Processing<a class="headerlink" href="#dataset-preparation-for-efficient-processing" title="Link to this heading">#</a></h3>
<p>We use <code class="docutils literal notranslate"><span class="pre">AUTOTUNE</span></code> to optimize the data loading and processing, and we configure the training and validation datasets for efficient shuffling, caching, and prefetching.</p>
</section>
<section id="normalization-layer">
<h3><span class="section-number">8.8.2. </span>Normalization Layer<a class="headerlink" href="#normalization-layer" title="Link to this heading">#</a></h3>
<p>The normalization layer rescales pixel values from the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code> to <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>. Normalizing the input helps the model converge faster during training by ensuring that the pixel values are small and consistent.</p>
</section>
<section id="model-structure-sequential-api">
<h3><span class="section-number">8.8.3. </span>Model Structure: Sequential API<a class="headerlink" href="#model-structure-sequential-api" title="Link to this heading">#</a></h3>
<p>The <strong>Sequential API</strong> allows us to stack layers in a linear fashion to define a basic CNN architecture.</p>
<section id="input-layer">
<h4><span class="section-number">8.8.3.1. </span>Input Layer<a class="headerlink" href="#input-layer" title="Link to this heading">#</a></h4>
<p>The input layer first rescales the images and specifies that each input image has dimensions <code class="docutils literal notranslate"><span class="pre">180x180</span></code> and 3 color channels (RGB).</p>
</section>
<section id="first-convolutional-block">
<h4><span class="section-number">8.8.3.2. </span>First Convolutional Block<a class="headerlink" href="#first-convolutional-block" title="Link to this heading">#</a></h4>
<p>The first block consists of a <strong>Conv2D layer</strong> with 16 filters (or kernels) followed by <strong>MaxPooling</strong>. The convolutional layer applies filters to the input image to extract low-level features (like edges), while the pooling layer reduces the spatial dimensions of the feature maps, making the model less sensitive to small changes and reducing the computational cost.</p>
</section>
<section id="second-convolutional-block">
<h4><span class="section-number">8.8.3.3. </span>Second Convolutional Block<a class="headerlink" href="#second-convolutional-block" title="Link to this heading">#</a></h4>
<p>The second block has 32 filters, allowing the network to learn more complex features like textures and patterns. Max pooling again reduces the dimensions of the output.</p>
</section>
<section id="third-convolutional-block">
<h4><span class="section-number">8.8.3.4. </span>Third Convolutional Block<a class="headerlink" href="#third-convolutional-block" title="Link to this heading">#</a></h4>
<p>In the third block, 64 filters are applied, allowing the model to detect higher-level features. The deeper we go into the network, the more abstract the features become, allowing the model to make more complex distinctions between classes.</p>
</section>
<section id="flattening-and-dense-layers">
<h4><span class="section-number">8.8.3.5. </span>Flattening and Dense Layers<a class="headerlink" href="#flattening-and-dense-layers" title="Link to this heading">#</a></h4>
<p>The <strong>Flatten</strong> layer converts the 2D feature maps into a 1D vector that is fed into the dense layers. The <strong>Dense(128)</strong> layer learns to combine the features detected in the convolutional layers. The final <strong>Dense(num_classes)</strong> layer outputs the classification logits for the <code class="docutils literal notranslate"><span class="pre">crab</span></code> and <code class="docutils literal notranslate"><span class="pre">rockfish</span></code> classes.</p>
</section>
</section>
<section id="layer-importance">
<h3><span class="section-number">8.8.4. </span>Layer Importance<a class="headerlink" href="#layer-importance" title="Link to this heading">#</a></h3>
<p>In the previous lesson, we used a single <strong>edge detection kernel</strong>, which was a manual, handcrafted filter. Here, the CNN automatically learns the best filters (or features) from the data. The layer structure is key to CNNs because each layer extracts more complex features, allowing the network to progressively understand the image in a more detailed and abstract way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A Basic Keras Model</span>

<span class="n">AUTOTUNE</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="n">normalization_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Rescaling</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>

<span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Rescaling</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="compiling-the-model">
<h2><span class="section-number">8.9. </span>Compiling the Model<a class="headerlink" href="#compiling-the-model" title="Link to this heading">#</a></h2>
<p>After defining the architecture of the CNN, the next step is to <strong>compile</strong> the model. Compiling involves specifying three key elements: the <strong>optimizer</strong>, the <strong>loss function</strong>, and the <strong>metrics</strong> used to evaluate the model’s performance.</p>
<section id="optimizer-adam">
<h3><span class="section-number">8.9.1. </span>Optimizer: Adam<a class="headerlink" href="#optimizer-adam" title="Link to this heading">#</a></h3>
<p><strong>Adam (Adaptive Moment Estimation)</strong> is a popular optimizer that combines the advantages of two other methods: <strong>AdaGrad</strong> (which works well with sparse gradients) and <strong>RMSProp</strong> (which adapts the learning rate based on recent gradients). Adam adapts the learning rate throughout training, making it a good default choice for most models.</p>
</section>
<section id="loss-function-sparse-categorical-crossentropy">
<h3><span class="section-number">8.9.2. </span>Loss Function: Sparse Categorical Crossentropy<a class="headerlink" href="#loss-function-sparse-categorical-crossentropy" title="Link to this heading">#</a></h3>
<p><strong>Sparse Categorical Crossentropy</strong> is the loss function used when we have multiple classes (in this case, <code class="docutils literal notranslate"><span class="pre">crab</span></code> and <code class="docutils literal notranslate"><span class="pre">rockfish</span></code>), and the labels are integer values. It measures how far the predicted probabilities are from the actual labels.</p>
<ul class="simple">
<li><p><strong>from_logits=True</strong>: This indicates that the output of the final layer is raw scores (logits) rather than probabilities. Since we haven’t applied a softmax activation in the final layer, logits will be converted to probabilities during the loss calculation.</p></li>
</ul>
</section>
<section id="metrics-accuracy">
<h3><span class="section-number">8.9.3. </span>Metrics: Accuracy<a class="headerlink" href="#metrics-accuracy" title="Link to this heading">#</a></h3>
<p><strong>Accuracy</strong> calculates the percentage of correct predictions made by the model. During training, both <strong>training accuracy</strong> and <strong>validation accuracy</strong> will be tracked to monitor how well the model is learning and generalizing.</p>
<p>By compiling the model, we set the stage for training, specifying how the model will optimize its weights, calculate loss, and measure success.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compile the Model</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model-summary">
<h2><span class="section-number">8.10. </span>Model Summary<a class="headerlink" href="#model-summary" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> function provides a concise overview of the model’s architecture. It displays the layer types, output shapes, and the number of parameters in each layer, giving you a quick understanding of the model’s structure and complexity.</p>
<p>This is useful to verify that the model is constructed as intended and to check the total number of trainable parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model Summary</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-the-model">
<h2><span class="section-number">8.11. </span>Training the Model<a class="headerlink" href="#training-the-model" title="Link to this heading">#</a></h2>
<p>In this step, we train the model using the <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code> function.</p>
<ul class="simple">
<li><p><strong>epochs=10</strong>: This specifies that the model will go through the entire dataset 10 times (or 10 training cycles). Each epoch allows the model to learn more patterns and improve its predictions.</p></li>
<li><p><strong>train_ds</strong>: The training dataset used for learning.</p></li>
<li><p><strong>validation_data=val_ds</strong>: The validation dataset used to evaluate how well the model generalizes to unseen data.</p></li>
</ul>
<p>The <strong>history</strong> object stores the training progress, including accuracy and loss, which we visualize in the next step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
  <span class="n">train_ds</span><span class="p">,</span>
  <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds</span><span class="p">,</span>
  <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-training-results">
<h2><span class="section-number">8.12. </span>Visualizing Training Results<a class="headerlink" href="#visualizing-training-results" title="Link to this heading">#</a></h2>
<p>This block of code visualizes the model’s <strong>training and validation accuracy</strong> and <strong>loss</strong> over the training epochs. These plots are crucial for understanding how well the model is learning and generalizing.</p>
<ul class="simple">
<li><p><strong>Training Accuracy and Validation Accuracy</strong>: These graphs show how the model’s accuracy improves during training and how well it performs on unseen validation data.</p></li>
<li><p><strong>Training Loss and Validation Loss</strong>: These graphs show how the model’s loss decreases during training. Loss is a measure of how well the model’s predictions match the true labels.</p></li>
</ul>
<p>Interpreting these graphs is one of the most important skills in CV. They help you assess whether the model is <strong>overfitting</strong> (performing well on training but poorly on validation) or <strong>underfitting</strong> (performing poorly on both training and validation).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize Training Results</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>

<span class="n">epochs_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_range</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_range</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and Validation Accuracy&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_range</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_range</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and Validation Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="making-predictions-on-a-new-image">
<h2><span class="section-number">8.13. </span>Making Predictions on a New Image<a class="headerlink" href="#making-predictions-on-a-new-image" title="Link to this heading">#</a></h2>
<p>After training the model, we can use it to make predictions on new, unseen images. In this example, we load a new image called <strong>“mystery.png”</strong>, preprocess it, and let the model predict whether it belongs to the “crab” or “rockfish” class.</p>
<p>Here’s a breakdown:</p>
<ul class="simple">
<li><p><strong>Load the image</strong>: The image is resized to the same dimensions (180x180) used during training.</p></li>
<li><p><strong>Preprocess</strong>: The image is converted to an array and expanded to match the input shape expected by the model (batch format).</p></li>
<li><p><strong>Make predictions</strong>: The model generates raw prediction scores for each class. We apply the <strong>softmax</strong> function to convert these scores into probabilities.</p></li>
<li><p><strong>Print the result</strong>: We output the predicted class (either “crab” or “rockfish”) along with the confidence level.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>
<span class="n">uploaded</span> <span class="o">=</span> <span class="n">files</span><span class="o">.</span><span class="n">upload</span><span class="p">()</span> <span class="c1"># call the upload method on files </span>

<span class="c1"># Load the image</span>
<span class="n">img_height</span> <span class="o">=</span> <span class="mi">180</span>  <span class="c1"># Set the appropriate target size</span>
<span class="n">img_width</span> <span class="o">=</span> <span class="mi">180</span>

<span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">uploaded</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
  <span class="n">image_path</span> <span class="o">=</span> <span class="n">fn</span> <span class="c1"># get the filename from the uploaded dictionary</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">))</span>
<span class="n">img_array</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img_array</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">img_array</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Create a batch</span>

<span class="c1"># Make predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img_array</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;crab&#39;</span><span class="p">,</span> <span class="s1">&#39;rockfish&#39;</span><span class="p">]</span>

<span class="c1"># Print the result</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;This image most likely belongs to </span><span class="si">{}</span><span class="s2"> with a </span><span class="si">{:.2f}</span><span class="s2"> percent confidence.&quot;</span>
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">score</span><span class="p">)],</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="LA4.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Understanding CV Metrics and Graphs</p>
      </div>
    </a>
    <a class="right-next"
       href="LA6.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>LA6: Object Detection with TensorFlow API</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">8.1. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">8.1.1. Learning Objectives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection-vs-image-classification">8.1.2. Object Detection vs. Image Classification</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries-overview">8.2. Libraries Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-description">8.3. Dataset Description</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-dataset-parameters">8.4. Understanding Dataset Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">8.5. Loading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-data">8.6. Visualizing the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-dataset-structure">8.7. Verifying Dataset Structure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-tensors">8.7.1. What Are Tensors?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-breakdown">8.7.2. Code Breakdown:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose">8.7.3. Purpose:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-basic-keras-model">8.8. A Basic Keras Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-preparation-for-efficient-processing">8.8.1. Dataset Preparation for Efficient Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-layer">8.8.2. Normalization Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-structure-sequential-api">8.8.3. Model Structure: Sequential API</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">8.8.3.1. Input Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-convolutional-block">8.8.3.2. First Convolutional Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#second-convolutional-block">8.8.3.3. Second Convolutional Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#third-convolutional-block">8.8.3.4. Third Convolutional Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-and-dense-layers">8.8.3.5. Flattening and Dense Layers</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-importance">8.8.4. Layer Importance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compiling-the-model">8.9. Compiling the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-adam">8.9.1. Optimizer: Adam</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-sparse-categorical-crossentropy">8.9.2. Loss Function: Sparse Categorical Crossentropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-accuracy">8.9.3. Metrics: Accuracy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-summary">8.10. Model Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model">8.11. Training the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-training-results">8.12. Visualizing Training Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions-on-a-new-image">8.13. Making Predictions on a New Image</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Atticus Carter
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>