{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jfG72-68Klv"
      },
      "source": [
        "# Ice Seal Classification using YOLOv11\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this lesson, we will train a YOLOv11 classification model on images of ice-associated seals from the NOAA Alaska Fisheries Science Center. These 640x640 images have been extracted from aerial photography using a separate ROI object detector. Due to the small size of the original imagery, individual seals cannot be identified with an adequate level of detail in a single pass. This necessitates a two-shot detection approach, where the first stage (the ROI object detector) localizes potential seals, and the second stage (the classifier) refines the identification at the species level. The dataset includes the following classes:\n",
        "\n",
        "bearded_pup\n",
        "bearded_seal\n",
        "ribbon_pup\n",
        "ribbon_seal\n",
        "ringed_pup\n",
        "ringed_seal\n",
        "spotted_pup\n",
        "spotted_seal\n",
        "unknown_pup\n",
        "unknown_seal\n",
        "After training the model, we will run inference on a folder (named transect) containing images of unknown seals. The aggregated predictions will then be visualized using heatmaps and stacked area plots to explore species distribution and ecological relationships.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Train a YOLOv11 classification model on marine seal images.\n",
        "- Evaluate the model's performance using standard metrics.\n",
        "- Perform inference on a transect folder of unknown seal images.\n",
        "- Aggregate and visualize predictions using heatmaps and stacked area plots.\n",
        "- Interpret the ecological relationships and species distributions from the visualization.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "The dataset comprises 640x640 images of ice-associated seals with the following distribution:\n",
        "- **bearded_pup:** 336 images\n",
        "- **bearded_seal:** 1537 images\n",
        "- **ribbon_pup:** 28 images\n",
        "- **ribbon_seal:** 185 images\n",
        "- **ringed_pup:** 43 images\n",
        "- **ringed_seal:** 2542 images\n",
        "- **spotted_pup:** 190 images\n",
        "- **spotted_seal:** 1329 images\n",
        "- **unknown_pup:** 313 images\n",
        "- **unknown_seal:** 648 images\n",
        "\n",
        "Each image file follows a naming convention (e.g., `100_bearded_pup.jpg`). Ensure that the dataset is organized and the paths are correctly set in your configuration file (`data.yaml`).\n",
        "\n",
        "Download the dataset here: https://huggingface.co/datasets/atticus-carter/NOAA_AFSC_MML_Iceseals_Classification/blob/main/640_yolo_classification_dataset.zip\n",
        "\n",
        "Download the secret transect here:\n",
        "https://huggingface.co/datasets/atticus-carter/NOAA_AFSC_MML_Iceseals_Transects/blob/main/transect_mystery.zip\n",
        "\n",
        "## Preparing the Environment\n",
        "\n",
        "Before starting, make sure you are using a GPU-enabled runtime. Run the cell below to check your GPU status.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRs67_jj8IhA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch7g4JDX8OOb"
      },
      "source": [
        "Next, install the required dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1SA6efgx8Sn-"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics matplotlib seaborn pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19r29PlI8Vf0"
      },
      "source": [
        "Now, import the required libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxf7xtaH8XCf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjn2j4APK-Wg"
      },
      "source": [
        "Then unzip your dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLNBs0bpNM0w"
      },
      "outputs": [],
      "source": [
        "!unzip /content/640_yolo_classification_dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZn3CvHy8cPP"
      },
      "source": [
        "## Training the Classification Model\n",
        "The following code trains the YOLOv11 classification model using the provided marine seal dataset. The model will be trained for 100 epochs with an image size of 640 pixels. Make sure the training configuration (e.g., class labels and paths) is correctly specified in your data.yaml file. Optionally you can configure your tensorboard now if you prefer to visualize metrics with it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jUbP-Ib48k2L"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/runs/classify/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC9oH2tE8YvJ"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv11 classification model\n",
        "model = YOLO(\"yolo11n-cls.pt\")\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=\"/content/640_yolo_classification_dataset\", epochs=300, patience=50, stream_buffer=True, imgsz=640)\n",
        "\n",
        "# Print training results\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms2TtAzl-GPj"
      },
      "source": [
        "## Assessing Model Performance\n",
        "Review the output images displayed above to assess the model's performance. Pay close attention to per-class metrics and overall performance indicators such as the F1 score, precision-recall curves, and confusion matrices. These visuals will help you identify which classes are performing well and where improvements might be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm_MpP0lRhrU"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"/content/runs/classify/train3/weights/best.pt\")\n",
        "results = model.val()\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMIDlomj-KzZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# Set the base directory\n",
        "base_dir = \"/content/runs/classify/train/\"\n",
        "\n",
        "# List of filenames to display\n",
        "filenames = [\n",
        "    \"labels.jpg\",\n",
        "    \"F1_curve.png\",\n",
        "    \"PR_curve.png\",\n",
        "    \"P_curve.png\",\n",
        "    \"R_curve.png\",\n",
        "    \"confusion_matrix.png\",\n",
        "    \"confusion_matrix_normalized.png\"\n",
        "]\n",
        "\n",
        "# Display each image\n",
        "for filename in filenames:\n",
        "    image_path = os.path.join(base_dir, filename)\n",
        "    display(Image(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2nlxL8L82uS"
      },
      "source": [
        "## Inference on the Transect Folder\n",
        "The transect folder is organized into subfolders representing every 100 meters along a 3km transect (e.g., 0m, 100m, 200m, â€¦, 3000m). Each subfolder contains ROI detection images from that segment. The code below traverses each subfolder, runs inference on all JPEG images within, and saves the resulting prediction images. It also aggregates the classification probabilities for later visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WJ_iA-gY2L8"
      },
      "outputs": [],
      "source": [
        "!unzip /content/transect_mystery.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFqVrtpv8zlN"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Load the pre-trained YOLOv11 model for inference\n",
        "model = YOLO(\"/content/runs/classify/train3/weights/ NOAA_AFSC_MML_Iceseals_Classification.pt\")  # using the pretrained model for inference\n",
        "\n",
        "# Define the path to the transect root folder\n",
        "transect_root = '/content/transect_mystery'\n",
        "\n",
        "# List all subfolders inside the transect folder (each subfolder represents a 100m segment)\n",
        "subfolders = sorted([os.path.join(transect_root, d)\n",
        "                     for d in os.listdir(transect_root)\n",
        "                     if os.path.isdir(os.path.join(transect_root, d))])\n",
        "\n",
        "# Initialize a dictionary to hold predictions per segment\n",
        "segment_predictions = {}\n",
        "\n",
        "for folder in subfolders:\n",
        "    # List all JPEG images in the current subfolder\n",
        "    images = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.jpg')]\n",
        "\n",
        "    # Check if the folder contains any images\n",
        "    if not images:\n",
        "        print(f\"Warning: Folder '{folder}' contains no JPEG images. Skipping...\")\n",
        "        continue  # Skip to the next folder if no images are found\n",
        "\n",
        "    # Run batched inference on the images in the current subfolder\n",
        "    results = list(model(images, stream=True))\n",
        "\n",
        "    # Collect predictions for aggregation\n",
        "    segment_probs = []\n",
        "    for result in results:\n",
        "        segment_probs.append(result.probs)  # Classification probabilities\n",
        "        # Save the result image with a modified filename indicating the segment folder\n",
        "        result.save(filename=os.path.join(folder, 'result_' + os.path.basename(result.path)))\n",
        "\n",
        "    # Store predictions for this segment folder in the dictionary using the subfolder name as key\n",
        "    segment_predictions[os.path.basename(folder)] = segment_probs\n",
        "\n",
        "# Now, segment_predictions is a dictionary keyed by subfolder names (e.g., \"0m\", \"100m\", etc.)\n",
        "# with a list of prediction probability arrays for each image in that segment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHhtRWHw9g6E"
      },
      "source": [
        "## Visualizing Transect Predictions\n",
        "We now aggregate the predictions from each 100m segment. For each segment, we compute the mean probability for each species and then visualize these aggregated predictions using a heatmap and a stacked area plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSv45ClF9eUm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Define class labels in the order corresponding to the model outputs\n",
        "class_labels = [\n",
        "    'bearded_pup', 'bearded_seal', 'ribbon_pup', 'ribbon_seal',\n",
        "    'ringed_pup', 'ringed_seal', 'spotted_pup', 'spotted_seal',\n",
        "    'unknown_pup', 'unknown_seal'\n",
        "]\n",
        "\n",
        "# Create a DataFrame to hold mean probabilities per segment\n",
        "segment_data = []\n",
        "\n",
        "for segment, probs_list in segment_predictions.items():\n",
        "    # Compute the mean probabilities for the segment if there are any predictions\n",
        "    if len(probs_list) > 0:\n",
        "        # Ensure each probability array is a NumPy array and average over the images\n",
        "        # Access the data attribute of the Probs object before converting to NumPy\n",
        "        segment_mean = np.mean([prob.data.cpu().numpy() for prob in probs_list], axis=0)\n",
        "        # Build a dictionary with the segment name and its mean probabilities\n",
        "        row = {\"Segment\": segment}\n",
        "        row.update({label: segment_mean[i] for i, label in enumerate(class_labels)})\n",
        "        segment_data.append(row)\n",
        "\n",
        "# Create a DataFrame from the aggregated segment data and sort by segment name\n",
        "df_segment = pd.DataFrame(segment_data)\n",
        "df_segment = df_segment.sort_values(by=\"Segment\")\n",
        "\n",
        "# Heatmap Visualization of Mean Classification Probabilities per Segment\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_segment.set_index('Segment'), annot=True, cmap='viridis')\n",
        "plt.title('Heatmap of Mean Classification Probabilities per Transect Segment')\n",
        "plt.xlabel('Species')\n",
        "plt.ylabel('Transect Segment (every 100m)')\n",
        "plt.show()\n",
        "\n",
        "# Assuming df_segment_sorted is your DataFrame\n",
        "# Sort the DataFrame by the 'Segment' column and assign it to df_segment_sorted\n",
        "df_segment_sorted = df_segment.sort_values(by=['Segment'])\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for col in df_segment_sorted.columns[1:]:  # Iterate through columns except 'Segment'\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=df_segment_sorted['Segment'],\n",
        "        y=df_segment_sorted[col],\n",
        "        name=col,  # Use column name for legend label\n",
        "        stackgroup='one',  # Group traces for stacking\n",
        "        mode='lines',      # Use lines for area fill\n",
        "        fill='tonexty'     # Fill area to the next trace\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Stacked Area Plot of Mean Species Distribution Along Transect',\n",
        "    xaxis_title='Transect Segment (every 100m)',\n",
        "    yaxis_title='Mean Probability',\n",
        "    legend=dict(x=1.05, y=1),\n",
        "    width=1000,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
