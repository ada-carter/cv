{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Augmenting Datasets\n",
    "\n",
    "In this lesson, we will focus on building and preparing datasets for deep learning models, discussing the rationale behind dataset creation and augmentation. We will learn how to perform batch processing of images with multiple augmentations, ensure that associated annotation files stay intact during dataset splitting, and how to choose the right augmentation techniques for different use cases. By the end of this lesson, you’ll have the skills to create a robust dataset pipeline.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand the rationale behind building datasets for deep learning, including the importance of augmentations and splits.\n",
    "2. Learn how to split datasets into training, validation, and test sets while ensuring associated annotation files (e.g., XML, JSON) stay intact.\n",
    "3. Use **batch processing** to apply different augmentations to large datasets.\n",
    "4. Customize augmentations based on the task (e.g., object detection, segmentation, classification).\n",
    "\n",
    "---\n",
    "\n",
    "## Rationale Behind Dataset Creation and Augmentation\n",
    "\n",
    "When training deep learning models, the **quality and diversity of your dataset** is critical. Here are a few reasons why augmenting and preparing datasets properly is important:\n",
    "\n",
    "- **Dataset Variety**: Real-world data is often limited, so augmentations help create variations in the data (rotation, flipping, brightness, etc.) to make models more robust.\n",
    "- **Data Splitting**: Properly splitting the dataset into training, validation, and test sets is important for ensuring that the model generalizes well. Validation sets help tune hyperparameters, while test sets evaluate final performance.\n",
    "- **Task-Specific Requirements**: Depending on the type of task (e.g., image classification vs. object detection), dataset augmentation strategies might differ. Object detection, for example, requires that the bounding box annotations remain consistent with the augmented images.\n",
    "\n",
    "## Introduction to Albumentations\n",
    "\n",
    "In **LA 2.5**, we explored how to manually apply image augmentations using **PIL** and **OpenCV**. While these foundational skills are critical for understanding how image transformations work, we now introduce **Albumentations**, a high-level image augmentation library designed for speed, simplicity, and flexibility.\n",
    "\n",
    "### Why Knowing the Foundations is Still Important\n",
    "\n",
    "Understanding how image augmentations work at a lower level provides key benefits:\n",
    "1. **Detailed Control**: Manual augmentation techniques using **PIL** or **OpenCV** give you full control over how transformations are applied. This is especially important when handling complex or custom tasks that might not be covered by high-level libraries.\n",
    "2. **Customization**: There are situations where highly specific augmentations are needed, such as when working with **custom image formats** or **non-standard data**. Knowing the underlying operations enables you to extend or modify augmentations beyond the capabilities of higher-level tools.\n",
    "3. **Better Debugging**: When a model's performance suffers from specific augmentations, it’s important to know how they work under the hood. Foundational skills help you troubleshoot issues when high-level libraries behave unexpectedly.\n",
    "\n",
    "While detailed control is important, for large-scale datasets like those used in marine science (e.g., images from ROVs, underwater drones, or satellite imagery), **Albumentations** provides a faster, more efficient way to perform bulk augmentations. Let's explore the syntax and key arguments of some Albumentations augmentations that are particularly useful for marine datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Useful Albumentations Augmentations for Marine Data\n",
    "\n",
    "Below is a list of key augmentations that can be used for marine datasets. Each transformation includes its **syntax**, **arguments**, and use cases.\n",
    "\n",
    ":::{note}\n",
    "Albumemtations is often installed as A to avoid the long (and annoying to say) name\n",
    "\n",
    "```python\n",
    "import albumentations as A\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Horizontal and Vertical Flips\n",
    "\n",
    "```python\n",
    "A.HorizontalFlip(p=0.5)\n",
    "A.VerticalFlip(p=0.5)\n",
    "```\n",
    "\n",
    "Here, 'p' is the probability of applying the flip\n",
    "\n",
    "**Use Case**: Flipping helps simulate different orientations of marine animals or objects. For example, mobile animals might be seen from different angles due to movement, and stationary organisms and geologic features can benefit from this augmentation due to variations in camera position. Flipping almost always increase variability, making it a very common augmentation.\n",
    "\n",
    "### Random Rotations\n",
    "\n",
    "```python\n",
    "A.Rotate(limit=45, p=0.7)\n",
    "```\n",
    "**Use Case**: When marine cameras tilt or rotate due to currents or vehicle movement, applying random rotations can make models more robust to different camera orientations.\n",
    "\n",
    "Here, 'limit' is the Maximum rotation angle (in degrees) in both directions.'p' is the probability of applying the rotation.\n",
    "\n",
    "### Brightness and Contrast Adjustments\n",
    "\n",
    "```python\n",
    "A.RandomBrightnessContrast(p=0.5)\n",
    "```\n",
    "\n",
    "\n",
    "MORE TO COME HERE, stopped at 1.29PM 9/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Dataset Splitting and Ensuring Annotation Integrity\n",
    "\n",
    "When splitting a dataset, it's important to ensure that the associated annotation files (such as **bounding boxes** for object detection or **segmentation masks**) remain aligned with the correct images after augmentation and splitting. Common dataset splits include:\n",
    "- **Training set**: Typically 70-80% of the data.\n",
    "- **Validation set**: Typically 10-15% of the data for tuning the model.\n",
    "- **Test set**: The final 10-15% for evaluating model performance.\n",
    "\n",
    "### Example: Splitting a Dataset with Associated Annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Directory paths\n",
    "image_dir = '/path/to/images'\n",
    "annotation_dir = '/path/to/annotations'\n",
    "train_dir = '/path/to/train'\n",
    "val_dir = '/path/to/val'\n",
    "test_dir = '/path/to/test'\n",
    "\n",
    "# Get image files\n",
    "images = [f for f in os.listdir(image_dir) if f.endswith('.png')]  # Change extension as needed\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "train_images, val_test_images = train_test_split(images, test_size=0.3, random_state=42)\n",
    "val_images, test_images = train_test_split(val_test_images, test_size=0.5, random_state=42)\n",
    "\n",
    "# Move images and their annotations to respective folders\n",
    "def move_files(image_list, target_dir):\n",
    "    for image in image_list:\n",
    "        # Move image\n",
    "        shutil.move(os.path.join(image_dir, image), os.path.join(target_dir, 'images', image))\n",
    "        \n",
    "        # Move associated annotation (assumes annotation has the same name but different extension)\n",
    "        annotation_file = image.replace('.png', '.xml')  # Adjust extension based on annotation type\n",
    "        shutil.move(os.path.join(annotation_dir, annotation_file), os.path.join(target_dir, 'annotations', annotation_file))\n",
    "\n",
    "# Move the split files\n",
    "move_files(train_images, train_dir)\n",
    "move_files(val_images, val_dir)\n",
    "move_files(test_images, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Processing and Custom Augmentations\n",
    "\n",
    "Once the dataset is split, you can apply **batch augmentations** to increase the diversity of the dataset. Depending on the task (classification, object detection, segmentation), certain augmentations may be more appropriate than others.\n",
    "\n",
    "### Example: Batch Augmentations with Albumentations (OpenCV backend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=45, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=25, p=0.5),\n",
    "    A.RandomCrop(width=128, height=128),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Batch process images in a folder\n",
    "input_dir = '/path/to/train/images'\n",
    "output_dir = '/path/to/augmented/images'\n",
    "\n",
    "def augment_images(input_dir, output_dir, transform):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for image_file in os.listdir(input_dir):\n",
    "        image_path = os.path.join(input_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        augmented = transform(image=image)['image']\n",
    "        \n",
    "        # Save augmented image\n",
    "        output_path = os.path.join(output_dir, image_file)\n",
    "        cv2.imwrite(output_path, augmented.numpy().transpose(1, 2, 0) * 255)  # Convert back to original range\n",
    "\n",
    "augment_images(input_dir, output_dir, transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use **Albumentations**, a fast and flexible image augmentation library, to apply various transformations to batches of images. The augmentation pipeline includes horizontal flipping, random rotations, brightness and contrast adjustments, and random cropping.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Customizing Augmentations Based on Task\n",
    "\n",
    "Different computer vision tasks require specific augmentations. Here are some augmentation strategies for common tasks:\n",
    "\n",
    "### 4.1 Object Detection\n",
    "\n",
    "When working on object detection tasks, it's important to ensure that the **bounding boxes** are adjusted appropriately with the image augmentations.\n",
    "\n",
    "#### Example: Augmenting Object Detection Data with Bounding Boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=45, p=0.7),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "# Example of augmenting an image with a bounding box\n",
    "image = cv2.imread('/path/to/image.png')\n",
    "bboxes = [[100, 150, 200, 250]]  # Example bounding box in PASCAL VOC format\n",
    "class_labels = ['crab']\n",
    "\n",
    "# Apply the augmentations\n",
    "augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "aug_image = augmented['image']\n",
    "aug_bboxes = augmented['bboxes']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **Albumentations** ensures that bounding boxes are modified along with the image, keeping the spatial relationships intact. The `bbox_params` argument specifies that we are using Pascal VOC format for bounding boxes.\n",
    "\n",
    "### 4.2 Image Segmentation\n",
    "\n",
    "For segmentation tasks, it’s crucial that **segmentation masks** undergo the same augmentations as the corresponding images.\n",
    "\n",
    "#### Example: Augmenting Segmentation Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=45, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "])\n",
    "\n",
    "# Augment both image and mask\n",
    "image = cv2.imread('/path/to/image.png')\n",
    "mask = cv2.imread('/path/to/mask.png', 0)  # Load mask as grayscale\n",
    "\n",
    "# Apply the augmentations to both image and mask\n",
    "augmented = transform(image=image, mask=mask)\n",
    "aug_image = augmented['image']\n",
    "aug_mask = augmented['mask']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, both the image and its corresponding segmentation mask are augmented together to ensure that the mask still matches the transformed image.\n",
    "\n",
    "### 4.3 Image Classification\n",
    "\n",
    "For classification tasks, standard augmentations like **random cropping**, **flipping**, and **brightness/contrast adjustments** are useful to improve model generalization.\n",
    "\n",
    "#### Example: Augmenting Image Classification Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=128, height=128),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=45, p=0.7),\n",
    "])\n",
    "\n",
    "# Apply augmentations to the image\n",
    "image = cv2.imread('/path/to/image.png')\n",
    "aug_image = transform(image=image)['image']\n",
    "\n",
    "# Save augmented image\n",
    "cv2.imwrite('/path/to/augmented_image.png', aug_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks, augmentations focus on changing the appearance and orientation of the image to help the model learn diverse features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Activity: Augmenting a Dataset of Crabs and Fish\n",
    "\n",
    "In this activity, you will create an augmented dataset using **randomcrab.zip** and **randomfish.zip**. Each ZIP file contains 35 images (180x180 pixels) of crabs and fish, respectively. Your task is to use image augmentations to expand the dataset to **400 images of crabs** and **400 images of fish**, ensuring the output images maintain the same file size (180x180) and keep the original file names.\n",
    "\n",
    "You’ll be provided with starter code that loads the images and applies basic augmentations. Your job is to customize the augmentation pipeline and generate the augmented images.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Download and extract the ZIP files**:\n",
    "   - **randomcrab.zip** contains 35 images of crabs.\n",
    "   - **randomfish.zip** contains 35 images of fish.\n",
    "\n",
    "2. **Augment each dataset**: Your goal is to generate a total of **400 images** for each class (crabs and fish) by applying various transformations (rotation, brightness, flips, etc.).\n",
    "\n",
    "3. **Ensure consistency**: Each output image must:\n",
    "   - Retain its original file name.\n",
    "   - Be the same size as the original (180x180).\n",
    "\n",
    "4. **Save the augmented images** in a directory called **augmented_data/crabs** for crabs and **augmented_data/fish** for fish.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code\n",
    "\n",
    "### Extracting and Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "# Paths to the ZIP files\n",
    "crab_zip = 'randomcrab.zip'\n",
    "fish_zip = 'randomfish.zip'\n",
    "\n",
    "# Extract ZIP files\n",
    "def extract_zip(file, extract_path):\n",
    "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "# Extract the crab and fish images\n",
    "extract_zip(crab_zip, 'crabs/')\n",
    "extract_zip(fish_zip, 'fish/')\n",
    "\n",
    "# Load the images\n",
    "crab_images = [os.path.join('crabs/', f) for f in os.listdir('crabs/') if f.endswith('.png')]\n",
    "fish_images = [os.path.join('fish/', f) for f in os.listdir('fish/') if f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, you'll define an augmentation pipeline to apply transformations. Choose augmentations that you think make the most sense for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation pipeline\n",
    "augmentations = A.Compose([\n",
    "#    A.HorizontalFlip(p=0.5),\n",
    "#    A.Rotate(limit=30, p=0.7),\n",
    "\n",
    "])\n",
    "\n",
    "# Create the output directories\n",
    "os.makedirs('augmented_data/crabs', exist_ok=True)\n",
    "os.makedirs('augmented_data/fish', exist_ok=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
