{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformers for Marine Computer Vision\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces Vision Transformers (ViTs) and their applications in marine computer vision. While Convolutional Neural Networks (CNNs) have been the dominant architecture for computer vision tasks, Transformers—originally designed for natural language processing—have recently shown remarkable performance in vision tasks. This lesson explores how ViTs can be applied to marine science challenges, offering new capabilities for analyzing underwater imagery.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "\n",
    "* Understand the fundamental architecture of Vision Transformers\n",
    "* Compare ViTs with traditional CNNs for marine image analysis\n",
    "* Implement a basic ViT model for marine species classification\n",
    "* Fine-tune pre-trained ViT models on marine datasets\n",
    "* Evaluate the performance of ViTs in challenging underwater conditions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Vision Transformers\n",
    "\n",
    "Vision Transformers (ViTs) represent a paradigm shift in computer vision. Introduced by Dosovitskiy et al. in their 2020 paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,\" ViTs adapt the Transformer architecture—originally designed for natural language processing—to visual tasks.\n",
    "\n",
    "Unlike CNNs, which process images through hierarchical convolutional layers, ViTs divide an image into fixed-size patches, linearly embed each patch, and process these embeddings with a standard Transformer encoder. This approach allows ViTs to capture global dependencies in the image without the inductive biases inherent in CNNs.\n",
    "\n",
    "### Key Advantages of ViTs for Marine Applications:\n",
    "\n",
    "1. **Global Context**: ViTs can capture long-range dependencies across the entire image, which is valuable for understanding complex marine scenes with varying scales and relationships.\n",
    "\n",
    "2. **Scale Invariance**: The self-attention mechanism in ViTs helps in recognizing objects regardless of their size in the frame, addressing the challenge of varying distances in underwater imagery.\n",
    "\n",
    "3. **Transfer Learning Capabilities**: Pre-trained ViTs can be effectively fine-tuned on smaller marine datasets, leveraging knowledge from large-scale pre-training.\n",
    "\n",
    "4. **Robustness to Occlusion**: ViTs have shown better performance in scenarios with partial occlusion, which is common in turbid water conditions.\n",
    "\n",
    "5. **Adaptability to Different Lighting Conditions**: The attention mechanism helps ViTs adapt to the variable lighting conditions encountered in underwater environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer Architecture\n",
    "\n",
    "Let's break down the architecture of a Vision Transformer:\n",
    "\n",
    "1. **Patch Embedding**: The input image is divided into fixed-size patches (e.g., 16×16 pixels). Each patch is flattened and linearly projected to create a patch embedding.\n",
    "\n",
    "2. **Position Embedding**: Since Transformers don't inherently understand spatial relationships, positional embeddings are added to provide information about the location of each patch.\n",
    "\n",
    "3. **Class Token**: A special learnable embedding called the class token is prepended to the sequence of embedded patches. The final state of this token serves as the image representation for classification tasks.\n",
    "\n",
    "4. **Transformer Encoder**: The sequence of embedded patches plus the class token is processed by a standard Transformer encoder, which consists of alternating layers of multiheaded self-attention (MSA) and MLP blocks, with layer normalization (LN) applied before each block and residual connections around each block.\n",
    "\n",
    "5. **MLP Head**: The final representation of the class token is passed through an MLP head to produce class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Simple Vision Transformer\n",
    "\n",
    "Let's implement a simplified version of a Vision Transformer to understand its core components. This implementation is for educational purposes and is not optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, in_channels, img_size, img_size)\n",
    "        x = self.proj(x)  # (batch_size, embed_dim, n_patches^0.5, n_patches^0.5)\n",
    "        x = x.flatten(2)  # (batch_size, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim // n_heads) ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, n_patches+1, dim)\n",
    "        batch_size, n_tokens, dim = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(batch_size, n_tokens, 3, self.n_heads, dim // self.n_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, n_heads, n_tokens, dim_per_head)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (batch_size, n_heads, n_tokens, n_tokens)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(batch_size, n_tokens, dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, drop=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio), dim, drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_size=224, patch_size=16, in_channels=3, n_classes=1000, embed_dim=768,\n",
    "        depth=12, n_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(embed_dim, n_heads, mlp_ratio, qkv_bias, drop_rate, attn_drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, in_channels, img_size, img_size)\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (batch_size, n_patches, embed_dim)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (batch_size, 1+n_patches, embed_dim)\n",
    "        x = x + self.pos_embed  # Add positional embedding\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Use the class token for classification\n",
    "        x = x[:, 0]  # (batch_size, embed_dim)\n",
    "        x = self.head(x)  # (batch_size, n_classes)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Vision Transformers\n",
    "\n",
    "While implementing a ViT from scratch is educational, for practical applications, it's more efficient to use pre-trained models. Let's see how to use a pre-trained ViT from the Hugging Face Transformers library and fine-tune it on a marine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install transformers datasets\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load a pre-trained ViT model\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a Marine Dataset\n",
    "\n",
    "For this example, let's assume we have a dataset of marine species images. We'll need to preprocess these images to match the input requirements of our ViT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess images for the ViT model\n",
    "def preprocess_images(examples):\n",
    "    images = examples[\"image\"]\n",
    "    processed_images = processor(images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    return {\"pixel_values\": processed_images, \"labels\": examples[\"label\"]}\n",
    "\n",
    "# Load a sample dataset (replace with your marine dataset)\n",
    "# For demonstration, we'll use a subset of the Oxford-IIIT Pet dataset\n",
    "dataset = load_dataset(\"oxford-iiit-pet\", split=\"train[:100]\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_dataset = dataset.map(preprocess_images, batched=True)\n",
    "processed_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(processed_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Display a sample image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(dataset[0][\"image\"])\n",
    "plt.title(f\"Label: {dataset[0]['label']}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the ViT Model\n",
    "\n",
    "Now, let's fine-tune our pre-trained ViT model on the marine dataset. We'll need to modify the classification head to match the number of classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the classification head for our number of classes\n",
    "num_classes = len(set(dataset[\"label\"]))\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "model.num_labels = num_classes\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "After fine-tuning, let's evaluate our model on a test set to see how well it performs on marine species classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = load_dataset(\"oxford-iiit-pet\", split=\"test[:50]\")\n",
    "processed_test_dataset = test_dataset.map(preprocess_images, batched=True)\n",
    "processed_test_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "test_dataloader = DataLoader(processed_test_dataset, batch_size=8)\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention Maps\n",
    "\n",
    "One of the advantages of Vision Transformers is the interpretability of attention maps. Let's visualize the attention patterns to understand what the model is focusing on when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract attention maps from the model\n",
    "def get_attention_maps(model, image):\n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass with output_attentions=True\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Extract attention maps\n",
    "    attention_maps = outputs.attentions  # This is a tuple of attention tensors\n",
    "    \n",
    "    return attention_maps\n",
    "\n",
    "# Get a sample image\n",
    "sample_image = test_dataset[0][\"image\"]\n",
    "\n",
    "# Get attention maps\n",
    "attention_maps = get_attention_maps(model, sample_image)\n",
    "\n",
    "# Visualize the attention map from the last layer\n",
    "last_layer_attention = attention_maps[-1].detach().cpu().numpy()\n",
    "\n",
    "# Average over attention heads\n",
    "avg_attention = np.mean(last_layer_attention, axis=1)[0]\n",
    "\n",
    "# Extract attention from the CLS token to all patches\n",
    "cls_attention = avg_attention[0, 1:]\n",
    "\n",
    "# Reshape to match the image patches\n",
    "patch_size = 16\n",
    "num_patches = int(np.sqrt(cls_attention.shape[0]))\n",
    "attention_map = cls_attention.reshape(num_patches, num_patches)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(attention_map, cmap=\"hot\")\n",
    "plt.title(\"Attention Map (CLS token to patches)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Vision Transformers in Marine Science\n",
    "\n",
    "Vision Transformers have shown promising results in various marine science applications. Here are some key areas where ViTs can be particularly beneficial:\n",
    "\n",
    "### 1. Marine Species Classification and Counting\n",
    "\n",
    "ViTs can effectively classify marine species in underwater imagery, even in challenging conditions with variable lighting, turbidity, and occlusion. Their ability to capture global context helps in distinguishing similar-looking species and counting individuals in crowded scenes.\n",
    "\n",
    "### 2. Coral Reef Monitoring\n",
    "\n",
    "The self-attention mechanism in ViTs can help identify subtle changes in coral reef health over time, detecting bleaching events, disease outbreaks, and recovery patterns with higher accuracy than traditional CNN-based approaches.\n",
    "\n",
    "### 3. Marine Debris Detection\n",
    "\n",
    "ViTs can be trained to detect and classify marine debris of various sizes and types, from microplastics to abandoned fishing gear, helping in pollution monitoring and cleanup efforts.\n",
    "\n",
    "### 4. Underwater Infrastructure Inspection\n",
    "\n",
    "For inspecting underwater structures like pipelines, cables, and offshore platforms, ViTs can identify structural anomalies, corrosion, and biofouling with high precision, reducing the need for human divers.\n",
    "\n",
    "### 5. Plankton Analysis\n",
    "\n",
    "ViTs can analyze plankton imagery from flow cytometers and microscopes, classifying different species and measuring their abundance, which is crucial for understanding marine food webs and ecosystem health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Coral Reef Monitoring with ViT\n",
    "\n",
    "Let's explore a hypothetical case study of using Vision Transformers for coral reef monitoring.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "A marine conservation organization needs to monitor the health of coral reefs across multiple locations. Traditional methods involve divers manually surveying the reefs, which is time-consuming, expensive, and limited in coverage. The organization has collected thousands of underwater images from autonomous underwater vehicles (AUVs) but needs an efficient way to analyze them.\n",
    "\n",
    "### Solution Approach\n",
    "\n",
    "1. **Data Collection**: Gather a diverse dataset of coral reef images, including healthy corals, bleached corals, diseased corals, and various coral species.\n",
    "\n",
    "2. **Data Annotation**: Annotate the images with labels for coral health status, species identification, and coverage percentage.\n",
    "\n",
    "3. **Model Selection**: Use a pre-trained ViT model as the backbone and fine-tune it on the coral reef dataset.\n",
    "\n",
    "4. **Training Strategy**: Implement a multi-task learning approach where the model simultaneously predicts coral health, species, and coverage.\n",
    "\n",
    "5. **Deployment**: Deploy the model on edge devices installed on AUVs for real-time analysis during surveys.\n",
    "\n",
    "### Results\n",
    "\n",
    "The ViT-based model achieves 92% accuracy in coral health classification, outperforming the previous CNN-based model by 7%. The attention maps reveal that the model focuses on specific coral features like polyp structure and coloration patterns when making predictions, providing insights into the model's decision-making process.\n",
    "\n",
    "The real-time analysis capability allows the AUVs to adaptively survey areas of interest, increasing the efficiency of monitoring operations by 60% and enabling the conservation organization to cover three times more reef area with the same resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges and Limitations\n",
    "\n",
    "While Vision Transformers offer significant advantages for marine computer vision, they also come with challenges:\n",
    "\n",
    "1. **Computational Requirements**: ViTs are computationally intensive, requiring substantial GPU resources for training and inference, which can be a limitation for deployment on resource-constrained underwater vehicles.\n",
    "\n",
    "2. **Data Hunger**: ViTs typically require larger datasets for training from scratch compared to CNNs, though this can be mitigated through transfer learning from pre-trained models.\n",
    "\n",
    "3. **Resolution Limitations**: Standard ViTs process images at a fixed resolution (e.g., 224×224 pixels), which may not be optimal for all marine applications, especially those requiring fine-grained details.\n",
    "\n",
    "4. **Domain Shift**: Pre-trained ViTs are typically trained on terrestrial images, leading to potential domain shift issues when applied to underwater imagery with different optical properties.\n",
    "\n",
    "5. **Interpretability Challenges**: While attention maps provide some interpretability, understanding the complex interactions between attention heads and layers remains challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "The field of Vision Transformers for marine applications is rapidly evolving. Here are some promising future directions:\n",
    "\n",
    "1. **Efficient ViT Architectures**: Development of more computationally efficient ViT variants like MobileViT and EfficientFormer that can run on edge devices deployed in marine environments.\n",
    "\n",
    "2. **Hybrid CNN-Transformer Models**: Combining the local processing capabilities of CNNs with the global context modeling of Transformers to create hybrid architectures optimized for underwater imagery.\n",
    "\n",
    "3. **Self-Supervised Learning**: Leveraging unlabeled marine imagery through self-supervised pre-training approaches like masked image modeling to reduce the dependency on large labeled datasets.\n",
    "\n",
    "4. **Multi-Modal Transformers**: Integrating multiple data modalities (visual, acoustic, environmental) through transformer-based architectures for more comprehensive marine environment understanding.\n",
    "\n",
    "5. **Adaptive Resolution Processing**: Developing ViT variants that can process images at variable resolutions or focus computational resources on regions of interest within high-resolution images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Vision Transformers represent a powerful new tool in the marine computer vision toolkit. Their ability to capture global context, adapt to varying conditions, and transfer knowledge from pre-training makes them particularly well-suited for the challenges of underwater imagery analysis.\n",
    "\n",
    "As the technology continues to evolve and computational efficiency improves, we can expect ViTs to play an increasingly important role in marine science applications, from species identification and ecosystem monitoring to underwater infrastructure inspection and marine debris detection.\n",
    "\n",
    "By understanding the fundamentals of ViT architecture and implementation, marine scientists can leverage these advanced models to gain new insights from visual data and address pressing challenges in ocean conservation and research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n",
    "\n",
    "2. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021). Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (pp. 10347-10357). PMLR.\n",
    "\n",
    "3. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 10012-10022).\n",
    "\n",
    "4. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9650-9660).\n",
    "\n",
    "5. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. arXiv preprint arXiv:2304.02643.\n",
    "\n",
    "6. Kiefer, B., Žust, L., Muhovič, J., Kristan, M., Perš, J., Teršek, M., ... & Lin, T. Y. (2025). 3rd Workshop on Maritime Computer Vision (MaCVi) 2025: Challenge Results. arXiv preprint arXiv:2501.10343v1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
