{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Image Super-Resolution and Enhancement with SRGAN in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction-cell",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this lesson, we will explore how to apply Super-Resolution Generative Adversarial Networks (SRGAN) to enhance marine images. Super-resolution is crucial in marine science for improving the quality of images taken in challenging environments, such as underwater scenes with low visibility or images captured from remote sensing devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-objectives-cell",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "By the end of this section, you will:\n",
    "\n",
    "- Understand the importance of super-resolution in marine science applications.\n",
    "- Learn how to prepare and load a dataset for super-resolution tasks.\n",
    "- Implement and train an SRGAN model using TensorFlow.\n",
    "- Understand the theory behind SRGAN and why it uses two convolutional networks.\n",
    "- Evaluate and interpret the results of super-resolution on marine images.\n",
    "- Reflect on the evaluation metrics and discuss the applicability of SRGANs in marine contexts.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-science-needs-cell",
   "metadata": {},
   "source": [
    "## Marine Science Applications of Super-Resolution\n",
    "Marine images often suffer from low resolution due to factors like water absorption, scattering, and limitations of underwater cameras. Enhancing these images is vital for tasks such as:\n",
    "\n",
    "- Identifying marine species and habitats.\n",
    "- Monitoring coral reef health.\n",
    "- Detecting illegal fishing activities.\n",
    "- Improving navigation and obstacle avoidance for underwater vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-download-cell",
   "metadata": {},
   "source": [
    "## Downloading the Dataset\n",
    "We will use a dataset called `digitalstill.zip`, which contains low-resolution and corresponding high-resolution marine images. Ensure you have this dataset available in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, PReLU, Add, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG19\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "from ipywidgets import interact, Dropdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation-cell",
   "metadata": {},
   "source": [
    "## Preparing the Environment and Data\n",
    "### Extracting the Dataset\n",
    "First, extract the contents of `digitalstill.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-dataset-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the uploaded dataset\n",
    "zip_file = '/content/digitalstill.zip'\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/digitalstill/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-cell",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the Data\n",
    "We will load the images and prepare them for training. We'll create low-resolution (LR) and high-resolution (HR) image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading-code-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "hr_dir = '/content/digitalstill/high_resolution/'\n",
    "lr_dir = '/content/digitalstill/low_resolution/'\n",
    "\n",
    "# Get image file names\n",
    "hr_images = sorted(os.listdir(hr_dir))\n",
    "lr_images = sorted(os.listdir(lr_dir))\n",
    "\n",
    "# Function to load images\n",
    "def load_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((128, 128))  # Resize for uniformity\n",
    "    img = np.array(img)\n",
    "    img = img / 127.5 - 1  # Normalize to [-1, 1]\n",
    "    return img\n",
    "\n",
    "# Load datasets\n",
    "X_hr = [load_image(os.path.join(hr_dir, img)) for img in hr_images]\n",
    "X_lr = [load_image(os.path.join(lr_dir, img)) for img in lr_images]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_hr = np.array(X_hr)\n",
    "X_lr = np.array(X_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "srgan-overview-cell",
   "metadata": {},
   "source": [
    "## Understanding SRGAN\n",
    "Super-Resolution Generative Adversarial Networks (SRGAN) are designed to produce high-resolution images from low-resolution inputs. They consist of two main components:\n",
    "\n",
    "- **Generator Network**: Attempts to create high-resolution images from low-resolution inputs.\n",
    "- **Discriminator Network**: Tries to distinguish between the generated high-resolution images and real high-resolution images.\n",
    "\n",
    "### Why Use Two Convolutional Networks?\n",
    "The use of two convolutional networks in SRGAN is rooted in the concept of Generative Adversarial Networks (GANs), where two models are trained simultaneously through an adversarial process.\n",
    "\n",
    "- **Generator (G)**: The generator's role is to generate images that are as close as possible to the real high-resolution images. It learns to map low-resolution images to high-resolution counterparts.\n",
    "- **Discriminator (D)**: The discriminator's role is to differentiate between the real high-resolution images and the images generated by the generator.\n",
    "\n",
    "This adversarial setup creates a minimax game:\n",
    "\n",
    "- The **generator** tries to minimize the difference between generated images and real images, effectively \"fooling\" the discriminator.\n",
    "- The **discriminator** tries to maximize its ability to correctly classify real and generated images.\n",
    "\n",
    "### Theoretical Background\n",
    "#### Generative Adversarial Networks (GANs)\n",
    "GANs are a class of machine learning frameworks where two networks contest with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set.\n",
    "\n",
    "- **Objective Function**:\n",
    "  $$\n",
    "  \\min_G \\max_D \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
    "  $$\n",
    "  Where:\n",
    "  - $G$ is the generator.\n",
    "  - $D$ is the discriminator.\n",
    "  - $x$ is a real data sample.\n",
    "  - $z$ is a random noise vector.\n",
    "\n",
    "#### Perceptual Loss\n",
    "SRGAN introduces a perceptual loss function, which is more effective than traditional loss functions (like Mean Squared Error) for capturing high-frequency details. It combines:\n",
    "\n",
    "- **Content Loss**: Measures the difference in high-level feature representations between generated and real images using a pre-trained network (e.g., VGG19).\n",
    "- **Adversarial Loss**: Encourages the generator to produce images that are indistinguishable from real images to the discriminator.\n",
    "\n",
    "#### Why Two Networks Improve Performance\n",
    "- **Adversarial Training**: The generator improves by learning from the discriminator's feedback, leading to more realistic and high-quality images.\n",
    "- **Feature Learning**: The discriminator learns to identify intricate details, pushing the generator to enhance these details in generated images.\n",
    "- **Balance**: The competition between the two networks helps in balancing the trade-off between blurriness and artifact introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-implementation-cell",
   "metadata": {},
   "source": [
    "## Implementing the SRGAN Model\n",
    "We will now define the generator and discriminator models. Both networks are convolutional neural networks (CNNs) but serve different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-theory-cell",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "The generator is responsible for upsampling low-resolution images to high-resolution images. It employs residual blocks and upsampling layers to reconstruct high-frequency details.\n",
    "\n",
    "#### Key Components:\n",
    "- **Residual Blocks**: Help in training deeper networks by mitigating the vanishing gradient problem.\n",
    "- **Upsampling Layers**: Increase the spatial dimensions of the feature maps.\n",
    "- **Activation Functions**: Use Parametric ReLU (PReLU) to allow for learning the activation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-model-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "def build_generator():\n",
    "    def residual_block(x):\n",
    "        res = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "        res = BatchNormalization(momentum=0.8)(res)\n",
    "        res = PReLU(shared_axes=[1, 2])(res)\n",
    "        res = Conv2D(64, kernel_size=3, strides=1, padding='same')(res)\n",
    "        res = BatchNormalization(momentum=0.8)(res)\n",
    "        res = Add()([res, x])\n",
    "        return res\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "    # Pre-residual block\n",
    "    x = Conv2D(64, kernel_size=9, strides=1, padding='same')(input_layer)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    # Store output for skip connection\n",
    "    skip_connection = x\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(16):\n",
    "        x = residual_block(x)\n",
    "\n",
    "    # Post-residual block\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Add()([x, skip_connection])\n",
    "\n",
    "    # Upsampling blocks\n",
    "    for _ in range(2):\n",
    "        x = UpSampling2D(size=2)(x)\n",
    "        x = Conv2D(256, kernel_size=3, strides=1, padding='same')(x)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Conv2D(3, kernel_size=9, strides=1, padding='same', activation='tanh')(x)\n",
    "\n",
    "    # Define model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Build generator\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discriminator-theory-cell",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator's role is to distinguish between real high-resolution images and the images generated by the generator. It is a binary classifier that outputs the probability of an image being real.\n",
    "\n",
    "#### Key Components:\n",
    "- **Convolutional Layers**: Extract features at different levels.\n",
    "- **LeakyReLU Activation**: Helps in learning non-linear relationships without dying neurons.\n",
    "- **Fully Connected Layers**: Aggregate extracted features to make the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discriminator-model-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "def build_discriminator():\n",
    "    def d_block(x, filters, strides=1, bn=True):\n",
    "        x = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(x)\n",
    "        if bn:\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        return x\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "    # Convolutional blocks\n",
    "    x = d_block(input_layer, 64, bn=False)\n",
    "    x = d_block(x, 64, strides=2)\n",
    "    x = d_block(x, 128)\n",
    "    x = d_block(x, 128, strides=2)\n",
    "    x = d_block(x, 256)\n",
    "    x = d_block(x, 256, strides=2)\n",
    "    x = d_block(x, 512)\n",
    "    x = d_block(x, 512, strides=2)\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Define model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Build discriminator\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-compilation-cell",
   "metadata": {},
   "source": [
    "### Compiling the Models\n",
    "We will compile the discriminator and the combined model. The discriminator is trained to classify images as real or fake, while the generator is trained to produce images that can fool the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compile-models-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Freeze the discriminator when training the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# VGG19 for perceptual loss\n",
    "vgg = VGG19(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "vgg.trainable = False\n",
    "\n",
    "# Define perceptual loss\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true = tf.keras.applications.vgg19.preprocess_input((y_true + 1) * 127.5)\n",
    "    y_pred = tf.keras.applications.vgg19.preprocess_input((y_pred + 1) * 127.5)\n",
    "    hr_features = vgg(y_true)\n",
    "    sr_features = vgg(y_pred)\n",
    "    return tf.keras.losses.MeanSquaredError()(hr_features, sr_features)\n",
    "\n",
    "# Input for generator\n",
    "img_lr = tf.keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "# Generate high-resolution images\n",
    "generated_hr = generator(img_lr)\n",
    "\n",
    "# Discriminator determines validity\n",
    "validity = discriminator(generated_hr)\n",
    "\n",
    "# Combined model (generator and discriminator)\n",
    "combined = Model(inputs=img_lr, outputs=[validity, generated_hr])\n",
    "\n",
    "# Compile the combined model\n",
    "combined.compile(loss=['binary_crossentropy', perceptual_loss],\n",
    "                 optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n",
    "                 loss_weights=[1e-3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-cell",
   "metadata": {},
   "source": [
    "## Training the SRGAN\n",
    "We will now train the SRGAN model using the prepared datasets. The training involves alternating between training the discriminator and the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-theory-cell",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "1. **Train Discriminator**:\n",
    "   - Use real high-resolution images and label them as real (1).\n",
    "   - Use generated high-resolution images from the generator and label them as fake (0).\n",
    "   - Update the discriminator's weights based on the loss.\n",
    "\n",
    "2. **Train Generator**:\n",
    "   - Use low-resolution images as input.\n",
    "   - The generator tries to produce high-resolution images that the discriminator classifies as real.\n",
    "   - The combined model's loss is a weighted sum of adversarial loss and perceptual loss.\n",
    "   - Update the generator's weights based on the combined loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "batch_size = 4\n",
    "sample_interval = 1000\n",
    "\n",
    "# Labels for real and fake images\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # ----------------------\n",
    "    #  Train Discriminator\n",
    "    # ----------------------\n",
    "\n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, X_hr.shape[0], batch_size)\n",
    "    imgs_hr = X_hr[idx]\n",
    "    imgs_lr = X_lr[idx]\n",
    "    \n",
    "    # Generate high-resolution images from low-resolution images\n",
    "    fake_hr = generator.predict(imgs_lr)\n",
    "    \n",
    "    # Train the discriminator (real classified as real and fake as fake)\n",
    "    d_loss_real = discriminator.train_on_batch(imgs_hr, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_hr, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # ------------------\n",
    "    #  Train Generator\n",
    "    # ------------------\n",
    "\n",
    "    # Train the generator (wants discriminator to label generated images as real)\n",
    "    g_loss = combined.train_on_batch(imgs_lr, [valid, imgs_hr])\n",
    "    \n",
    "    # Print the progress\n",
    "    if epoch % sample_interval == 0:\n",
    "        print(f\"[Epoch {epoch}/{epochs}] [D loss: {d_loss[0]:.5f}, acc: {100*d_loss[1]:.2f}%] [G loss: {g_loss[0]:.5f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-cell",
   "metadata": {},
   "source": [
    "## Evaluating and Interpreting the Results\n",
    "We will now evaluate the performance of the trained SRGAN model by visualizing some results and discussing evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-theory-cell",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for SRGAN\n",
    "Evaluating super-resolution models involves both quantitative metrics and qualitative assessments.\n",
    "\n",
    "#### Quantitative Metrics:\n",
    "- **Peak Signal-to-Noise Ratio (PSNR)**: Measures the ratio between the maximum possible power of a signal and the power of corrupting noise. Higher PSNR indicates better quality.\n",
    "- **Structural Similarity Index (SSIM)**: Measures the similarity between two images. Values range from -1 to 1, with 1 indicating perfect similarity.\n",
    "\n",
    "#### Qualitative Assessment:\n",
    "- **Visual Inspection**: Assessing the visual quality of the generated images for artifacts, blurriness, and realistic textures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-code-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for evaluation\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random set of images\n",
    "idx = np.random.randint(0, X_lr.shape[0], 3)\n",
    "imgs_lr = X_lr[idx]\n",
    "imgs_hr = X_hr[idx]\n",
    "\n",
    "# Generate high-resolution images\n",
    "generated_hr = generator.predict(imgs_lr)\n",
    "\n",
    "# Rescale images for visualization\n",
    "imgs_lr_vis = 0.5 * imgs_lr + 0.5\n",
    "generated_hr_vis = 0.5 * generated_hr + 0.5\n",
    "imgs_hr_vis = 0.5 * imgs_hr + 0.5\n",
    "\n",
    "# Calculate metrics\n",
    "psnr_values = [psnr(imgs_hr[i], generated_hr[i], data_range=2.0) for i in range(3)]\n",
    "ssim_values = [ssim(imgs_hr[i], generated_hr[i], multichannel=True, data_range=2.0) for i in range(3)]\n",
    "\n",
    "# Plot the results\n",
    "titles = ['Low-resolution', 'Generated High-resolution', 'Original High-resolution']\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for i in range(3):\n",
    "    axs[i, 0].imshow(imgs_lr_vis[i])\n",
    "    axs[i, 0].set_title(titles[0])\n",
    "    axs[i, 1].imshow(generated_hr_vis[i])\n",
    "    axs[i, 1].set_title(f\"{titles[1]}\\nPSNR: {psnr_values[i]:.2f}, SSIM: {ssim_values[i]:.3f}\")\n",
    "    axs[i, 2].imshow(imgs_hr_vis[i])\n",
    "    axs[i, 2].set_title(titles[2])\n",
    "    for j in range(3):\n",
    "        axs[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection-questions-cell",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "Consider the following questions to reflect on the results and deepen your understanding:\n",
    "\n",
    "1. **Detail Enhancement**: Observing the generated images, do you notice a significant improvement in details compared to the low-resolution inputs? Provide specific examples.\n",
    "2. **PSNR and SSIM Metrics**: How do the PSNR and SSIM values correlate with the visual quality of the images? Are higher values always indicative of better quality in the context of marine images?\n",
    "3. **Artifacts**: Are there any artifacts introduced in the generated images? What might be causing them, and how could they affect marine image analysis?\n",
    "4. **Color Accuracy**: Does the color reproduction in the generated images match the original high-resolution images? How important is color accuracy in marine applications?\n",
    "5. **Limitations of SRGAN**: Based on your observations, what are some limitations of using SRGANs for marine image enhancement? Consider factors like computational resources, training data requirements, and potential overfitting.\n",
    "6. **Alternative Approaches**: If SRGANs are not suitable for certain marine applications, what alternative methods could be used for image enhancement? Discuss their potential advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-cell",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this lesson, we explored how super-resolution can significantly enhance marine images, aiding in various scientific and monitoring tasks. We implemented an SRGAN model in TensorFlow, trained it on marine images, and evaluated its performance using both quantitative metrics and qualitative assessments. We also delved into the theoretical underpinnings of SRGAN, understanding the roles of the generator and discriminator networks.\n",
    "\n",
    "### Key Takeaways\n",
    "- **Applicability**: SRGANs can improve image resolution, but their effectiveness depends on the quality and quantity of training data.\n",
    "- **Evaluation**: Metrics like PSNR and SSIM are helpful but should be complemented with visual inspections.\n",
    "- **Challenges**: High computational costs and the need for extensive training data can limit the use of SRGANs in marine contexts.\n",
    "- **Alternatives**: Other methods may be more suitable depending on the specific marine application and available resources.\n",
    "\n",
    "Super-resolution techniques like SRGAN hold great promise for improving the quality of marine imagery, leading to better analysis and decision-making. However, it's essential to consider their limitations and evaluate whether they are the best tool for a given task in marine science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
